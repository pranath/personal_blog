---
layout: post
title:  "The Third Attractor: Part 2"
categories: [third-attractor]
image: assets/images/third_attractor.png
toc: true
---
In this series of articles we will be exploring the ideas around the ['Third Attractor'](https://blog.livingdatalab.com/categories#third-attractor) - a concept relating to the future of our species and civilisation developed by Daniel Schmachtenberger. A brief introduction to these ideas can be found [here](https://blog.livingdatalab.com/the-third-attractor-introduction/) and an article on the first podcast [here](https://blog.livingdatalab.com/the-third-attractor-part1/).

This article will explore the content of the second podcast laying out Daniel's ideas.

## Introduction

Danel's intention and hope for this work is that it gives a more holistic overview of they key challenges our civlisation faces, and what may be some of the characteristics of possible solutions to these.

As he describes:

>My hope with this content today is that we can give a bit of an overview of some of the
catastrophic risks that the world currently faces, why the solutions to some of those risks can actually make other of the risks worse, and why the solutions to the risk landscape as a whole can create dystopias.
>
>Because to have the power to be able to check all the catastrophic risks can become a very concentrated power that is itself is hard to check. All of these risks are unsolvable without solving the patterns of human behavior that give rise to them. 
>
>But the patterns of human behavior that give rise to them can be named and can be progressively better addressed and that in doing so there is a possibility to create a attractor for the world that is not catastrophes or dystopias, in which we have the responsibility and wisdom adequate individually and collectively to steward the power of the exponential technology that we are creating.
>
>The hope of this as a introduction is that people might feel that it helps them make sense of what feels like lots of different problems in the world in a way that is hopefully clarifying and integrating and they can have more people engage in the kind of innovation and collective intelligence towards solutions to the deepest underlying dynamics.

For this second part of the podcast Daniel says:

>This part covers a lot of topics that I haven't covered in other places yet like a model of hyperagents, the great game of power in the 21st century, who the actors are in that game...a proposed relationship between infrastructure, social structures, and superstructure (culture) that addresses the meta crisis, a rough vision of the future of education, and a beginning overview of some of the broad brush strokes solutions to the meta-crisis.  

The rest of this article is closely based on the transcript of the second podcast speaking in Daniels voice.

## Understanding the problem

So we can see that when we come back to try to prevent catastrophes and dystopias, what is the third attractor what are some of the criteria?

We're just introducing the frameworks of the problem space so it would even give us the insights of how to think about tha. This famous quote by Charles Kettering that a problem fully understood is half solved, there's a kind of corollary of that which is if a problem fully understood is half solved, a problem not well understood is probably insolvable because i don't even know what the real problem is.

I think so often, when we think we know what the problem is that is a result of lots of other things interconnected with other things. Where the solution will end up causing other problems, it then becomes the problem.

This is why some protracted and deep analysis into what is actually the problem space we're looking at can enable solutions that might be adequate and viable given that our problem solving processes would have driven so many of our worst problems is an important thing. So we're kind of getting into that first.

But one of the things we can see as a criteria of a third attractor is it whatever that world system is and even saying world system is kind of like assuming some things that i don't want to assume. I just don't have better terminology yet but t can't only be a local thing as we've mentioned. So for whatever the kind of global collective intelligence system is, it has to be able to prevent all human induced catastrophes.

Now we can get into non-human use catastrophes as well and of course it should have the best capacity to mitigate those as best as it can but certainly preventing the human induced ones. It takes a lot of capacity i.e power to be able to prevent all those. If you're going to prevent the weaponization of exponential tech that is radically decentralized, do you need something like surveillance or do you need to completely change say the nature of how goods and services are provisioned and acquired? 

That individuals don't get access to the kinds of technology that could make catastrophic things, only communities or groups of people do. It takes something that is really a significant kind of power and in a way also an imposition on individual human freedoms. But if you allow maximum individual freedom multiplied by exponentially catastrophic capability that's probably a problem right?

You think about cells in the body and if a particular cell gets damaged and it becomes a cancer cell and then releases oncogenes it would create more cancer cells around and mastasize. The body has to keep it from doing that it has to say no you actually don't get the freedom to replicate faster and consume more glucose faster that'll end up killing everything.

So it'll try to repair that cell and if it can't it'll kill that cell because otherwise if it gets to continue doing that you'll have the maximum number of cancer cells the moment before the person dies. Then all the cancer cells die itself terminating its own short-term proliferation leads to its own self-termination. So whereas the other cells being the non-cancer cells being in some kind of coordinated dynamic that doesn't replicate as fast and doesn't get glucose as fast and doesn't have as much kind of unchecked metastasis also end up living a lot longer, and being part of something that has more orderly complexity.

It's not a perfect analogy, but it is not a useless analogy. So how do we bind individual freedom to do anything when the power of individual freedom becomes as significant as it is becoming. How do we do that in a way where the force that binds it right the law the monopoly of violence the whatever has a jurisprudence that is based on the wisest kind of ethics? Then how do we ensure that there is not corruption of the system of power so we can be able to stop market races looks like a lot of control over companies by something that looks like a regulatory apparatus to be able to stop international tragedy of the commons.

This looks like something like a lot of influence over nation states by some kind of international agreement process and the ability to stop decentralized catastrophic weapons, even the accidental versions look like the ability to like there's alot of power so to prevent the catastrophes does require some kind of coordinated power. 

That's significant to be able to prevent the dystopias means that the power that would be checking those other things has to have checks and balances on itself that are able to ensure that it is not itself a kind of corrupt power system subject to the same types of game theory.

So how might that happen? when we think about the problem of collective action problems in the multipolar trap it becomes easy to say well this is why a singleton some kind of top-down authority is so useful because it can prevent that problem. So this is why a you know a monarch of the world might be a good idea.

## Benevolent dictatorship does'nt work

So the idea of let's create some ethically programmed benevolent ai overlord is an easy idea to come up with. Or one can imagine a scenario like well maybe we'll make a person or a government or something that'll do that. An enlightened dictatorship seems like a good idea for dealing with these highly technologically powerful but otherwise rivalrous primates. 

There's a very good animation on Youtube called [Rules for Rulers by CGP Grey](https://www.youtube.com/watch?v=rStL7niR7gs) where he overviews stuff that's discussed in places like [the dictator's handbook](https://www.goodreads.com/book/show/11612989-the-dictator-s-handbook) on why enlightened benevolent dictatorship is not really a stable thing. Briefly i mean it's kind of obvious, it's a different case of a multi-polar trap. 

It's a multi-polar trap within a unified power system. If the ruler if the dictator is too benevolent then of course they will fail to malevolent external competitors who are willing to do particularly up stuff. If this person isn't willing to kill some civilians in war and the other side is or whatever it is. But not just externally internally, there will be some more malevolent agents within their own system that want power that are attracted to the power of dictatorship and who are willing to lie manipulate and do whatever needed. 

So of course the benevolent one will lose to the malevolent one unless they engage in the dynamics of effective power enough to not lose. Which means they are not as benevolent and then when you start to factor the incentive of that power that whoever is most oriented to the power and most willing to do whatever it takes to get there creates more and more need for whoever would stay there trying to be benevolent, to be less purely benevolent.

You end up seeing this kind of internal multipolar trap erosion and so this is where you get dystopias the benevolent dictator has to be increasingly less purely benevolent and increasingly rationalize how to mitigate malevolence through actions that look closer to malevolence. They will either become psychologically damaged by that enough that they become actually malevolent themselves or they rationalize that they are more benevolent in their spirit than the other guy. As a result they have to maintain this position of power which means they have to do the things required in order to do that. 

So that is itself a kind of multi-polar trap, it's a kind of game theoretic race to a sub-optimal outcome and this relates to the thing that we were saying earlier about why in a kind of pure market dynamic you will get a power law distribution of wealth. Some people will be better at wealth accumulation than others, and as they get more power differential expressed as capital, the capital is power. It's the ability to put more people under your employment have more technology more resources and the cash what the currency is that land or lumber or iron or whatever isn't it is maximally liquid, maximal optionality, for any kind of power i might want.

There is an orientation towards being able to have this maximum optionality and it's something that has no real value but optionality for any source of real value. So it's more desirable than anything with real value to the point that will actually destroy things of real value to create the currency that gives us optionality on some you know other value set. 

But in a market system some people do a better job of being able to create a profit and then be able to reinvest that to create more return on investment. And the increased power differential they get gives them more ability to both protect and advance their power. This is what books like [Piketty's Capital in the Twenty-First Century](https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century) outlined some of the dynamics.

One way to think about it is let's say we had two different thought experiment civilizations, they had the exact same distribution of natural resources and human talent. At t equals zero same number of people and the same amount of currency indexing all those resources and the only difference between the two is some kind of law. We're not going to talk about how this law is created and how it's doesn't get changed by the market just magically pretend this law works where in in group a normal market dynamics as we traditionally think of them.

But in this group b, there's a law that binds wealth inequality somehow let's say there's a 5x binding on wealth inequality so the richest person can't be more than 5x richer than the poorest person in the system. And we just let both systems play out from t equals 0. System a will end up under most scenarios, i have not ran this experiment but as a thought experiment system a will beat system b when they come into competitive dynamics with each other in most scenarios.

Why? because system a will end up developing a power law distribution where a tiny percentage of the people own most of the wealth and system b won't have that. As a result the power is not highly concentrated in system b and so it can't make collective action without doing collective choice making. Collective choice making is slow and you have to deal with all the conflicting interests and whatever the person over here who has all the money.

Or the couple people who have all the money can make a choice by themselves and then implement it through everything else that is under their control and ownership very very fast. Maybe the few people who have most of the money as oligarchs can coordinate with each other much easier than everybody here can coordinate with each other. So as a result when it comes into a direct competition of many times that system wins but again wins in a rivalrous context and is good generally in some broader definitions of good aren't the same thing.

So if you wanted that to be different you'd have to have that these many people over here could coordinate with each other as effectively and with time scales that are appropriate. To be able to not lose to the coordination of a few people over here that would require collective intelligence systems that could and of course everybody over here has way more information.

The collective intelligence is a lot of intelligence that cannot be adequately concentrated through a hierarchy and chain of command to the top. So if you had a situation where there was more total buy-in here, and more total intelligence if it could be adequately harvested and you could have right kinds of coordinations of course. It could be we've just not figured out how to do that.

So now if we were to talk about what is a third attractor, it's not going to be a top-down singleton which will end up being dystopic. It's not going to be many different actors caught in multiple traps with each other. It's going to be something where the system as a whole has the power to be able to check the catastrophic dynamics within this, but without having power consolidation itself.

Where the system has a kind of coordination across decentralized capacity so centralized emergent coordination across lots of decentralized capacities. This is of course the idea of what democracy is trying to achieve. We can see why it was partially successful, and then why it failed and got captured under advancements of technology and complexity and financing like that. But can we build something with the very best of the knowledge and information technologies available to us now that makes coherent choice making?

That factors the total collective intelligence and total collective agency with the speed and coherence of what a small number of agents would have and yet without any kind of centralized power? Well we can say that something like that is part of the criteria set of a third attractor of a desirable civilization that is not catastrophes or dystopias.

## Hyper agents

So we were talking about why power law distributions of wealth pretty naturally emerge in market scenarios, and we're also talking about where people in positions of power don't get to be maximally benevolent and maintain those positions of power for very long in most scenarios. These are a couple examples that bring up a concept that i think is worthwhile that we can think of as hyperagency.

It's pretty easy to see that throughout history a very small number of people have very outsized influence on the shape of society and civilization as a whole and obviously a lot of history is focused on this kind of great man of history narrative. There are lots of critiques of why those people were only able to do what they did because of what so many other people did that they took credit for parasitize whatever which is of course also partially true. But nonetheless the capacity to take credit for a parasitized harness whatever it is lots of people. 

Whether you have a negative or a positive take on it the ability to harness lots of people's activity aligned with a particular agenda direction is not a equally distributed capacity. So if you think about somebody like a genghis khan or an alexander the great compared to most everybody else they ever encountered they had a outsized influence on the shape of the world.

The nature of which genetics made it through what technologies made it through what cultures made it through like radically outsized and in a way that is not true for any other animal this is a unique thing to sapiens. It's worth noticing it's actually, a very important digression is of why it's very inappropriate for humans to model themselves as apex predators - which is a very common thing. In the kind of social darwin world social darwinism way. 

If you think about actual apex predators other than humans there's a few things about them versus humans that are quite obviously different. If you think about say a polar bear or a lion or a orca like really really really apex predators how much harm can they cause to their own environment if they really want to, and kind of go crazy. How much harm can a polar bear cause to the arctic or the environment? like just not that much. They just can't do that much harm. 

If an orca just went crazy trying to eat all the fish that it could - it couldn't in its whole life do what a driftnet can do in one catch. So you're like whoa - a drift net compared to an orca is not an apex predator thing like that's a totally different thing.
You look at areas that are completely desertified that used to be ecosystems, orca can't do that polar bear can't do that - lion can't do that. Clearly we have a capacity to affect the world that is not like apex predators.

Also apex predators how asymmetric they are relative to their environment is way less they than the fish - which get away from the orca a lot of the time as often as the orca catches them. The animals in the factory farm don't get away very often. We have an asymmetry of power mediated by technology and mediated by our capacity for abstraction, which allows us to do tech and language and coordination systems. 

Where the asymmetry of capacity of humans relative to everything else is so much that we can literally genetically engineer and make new life forms, we can extinct whole life forms, we can extinct whole ecosystems we can create the anthropocene where the human built world is the most significant geologic force affecting the surface of the earth for a period of time. 

So the idea that we model ourselves as apex predators that are just seeking to be an apex position is an extinctionary thought because that is continuing to drive the multi-polar trap arms race kind of thinking. Where we fundamentally are not that thing, we have something much less like the power of an apex predator and much more like the power of nature itself that can create new species and destroy species and make new ecosystems. So as something that has something closer to the power of nature itself being stewards of all that is obligate or we self-destruct in the process of destructing that which we depend upon. 

It's also quite important to understand in terms of epicural theory not only do humans have a radical asymmetry to everything else that apex predators don't have - the asymmetry that some humans have relative to other humans is way more than happens anywhere else. So the alpha lion is not a thousand times more lethal than the median lion or the weakest lines you know like the alpha lion is not twice as lethal as two average male lions. Maybe 1.2 x more at most and that for a fairly short period of time till somebody else overthrows them. 

So the total amount of power asymmetry within the species is pretty little and the power symmetry between the species pretty little, how much more influential was genghis khan than other people or say putin or xi jinping or a u.s president? Today in terms of the nuclear capacity they could deploy relative to the harmful capacity that I could deploy or you could deploy billions of times trillions of times.

So it's pretty easy to get that human society is not just the result of the way that humans are that have like certain kinds of traits, and there's some gaussian distribution of those traits and that the emergent property of those traits determines society. There are a fairly small number of people who have an outsized influence relative to everybody else who end up also creating the conditioning environments that determine what is within the gaussian distribution that everyone else is conditioned to.

Those small number of people have psychological and behavioral traits that are mostly outside of the gaussian distribution of all the other traits. That sociology looks at for humans meaning the nature of their motivational schemas, their intelligence their coordination capacities their ethical schemas are two or three or four standard deviations outside of normal, but two or three or four standard deviations outside of normal level of impact as well.

So what this means is when you kind of do sociological analysis on how people are and then try to make sense of the world from that there's a pretty big gap and so understanding. That there's an outsized influence of some people - hyper agents we can call them and that they are different and pretty significant in fundamental ways, and in ways that then influence how everybody else is. I think this is a very important concept to try to make sense of the world.

What makes someone a hyper agent? one way to think about it doesn't necessarily mean that they think about it this way is that they are oriented to and good at maximizing returns on agency. So what makes somebody a good investor is they maximize returns on investment that's one type of agency because money buys certain types of choice making capacity, but they're obviously people who have more total influence in the world relative to the amount of money they have than others. This is because they have it of other types. Does the number of twitter or social media followers equal a certain kind of currency? Totally. Does the amount of kind of backend political connections equal a type of influence capacity? Sure. 

Somebody like a kissinger would have had an outsized hyper agent influence on the world relative to the total amount of money he personally owned because of the total amount of money and agency and whatever that he had, had some influence over so this. So this is not the total degree of hyper agency, and total amount of money don't correlate obviously. Someone can also inherit money and be quite bad at knowing how to deploy it effectively.

When we talk about hyperagency we think about a orientation to apply their agency which means their choices their money their resources, their network - in ways that increase their total agency, and not simply that they're oriented that way but that they are good at doing it at scale. Because there are plenty of middle managers that do everything they can to play power games and kind of advance their own power and agency. They just don't get to the position of world leadership because they're just not that good at it. 

Even individual bullies that are oriented that way and but again are not that good at being able to leverage lots of other people and lots of technology in the process. 

So a tier one hyper agent, a top tier is someone who can coordinate the agency of lots of other people and extend it through a lot of technology, both of which are kind of extensions of understanding the dynamics of power itself and how to be able to extend their power through the augmenting capacity of other people and technologies.

It's very easy for hyper agents to be in a certain kind of multi-polar trap dynamic with other hyper agents. Where if i'm in such a position and i don't maximize my return on agency in a particular way because i think this activity would be unethical, but someone else who doesn't have the same value constraints will maximize their returns on agency. Then they'll have more influence over the whole system and either for my own egoic reasons i want to or even for ethical reasons i think i'm better to be in that position. Then again i won't feel like i have real choice it'll feel like i am obligated to maximize my returns on agency so as to be able to either achieve the thing i personally want, or rightly lead the world because i'm best fit to do that thing.

We can think now about our hyper agents as the only thing that has effective agency at scale that are involved in multi-polar traps.

## The great game of power

It's a very interesting question to say to think about who are the strategic actors most influencing the dynamics of the world. If we think of kind of the great game of power on the global stage who are the players in the great game of power both who are the strategic actors and what kinds of entities even are they?

I would say that the 21st century example or instantiation of the great game of power is different pretty fundamentally than the 20th century was and which was different than you know previous time periods. When we think about 20th century who got nuclear capability and who got things like global reserve currency and ability to influence the intergovernmental organizations that influence the world. Those were like pretty big aspects of the great game of power in the 21st century and obviously then who not only got the nuclear capacity, but the ability to control who else got it

We can see how that play worked in the period of exponential technology and network dynamics where you get radically fast speed to new types of power that didn't exist before much faster to global scale than previously occurred, and natural kinds of monopolies that can emerge. I would say that a major feature of the great game of power in the 21st century is the race to supremacy or dominance in one or more categories of exponential tech. 

The race to functional monopolistic capability of things that have network dynamics and the application of the dominance and exponential tech and networks to elements of real power in the world. Whether that is trying to maintain or develop dominance of some fundamental thing. We can see that part of the 100-year planning process that china has that smart thing to do involves having acquired something like 94% of all the mines that produce rare earth metals in the world which are needed to make the semiconductors that make computation.

Then one of the many things that are important reasons to gain control over taiwan has to do with control over semiconductor manufacturing. So you can see that that's kind of a play for a choke point. Mines are very deep to it of the capacity to create computation to create chips. Whether we're talking about that kind of the race to 5g and who owns the kind of satellite networks that will be able to mediate the communications, or the race to who has the platforms that have the most users in the most time on site that end up equaling maximum human influence. 

The race to who gets the first digital currency that gets enough scale that it becomes something like a global reserve currency. So i think the great game of power in the 21st century looks a lot like races for dominance in exponential tech categories, in network dynamics and the application of those two different types of real power in the world. I think that there are a number of players that know that is the game and that are actively playing. It's not a large number i think that there are people who are the heads of some of the largest nation states in the world who don't even know, wouldn't even say that that's the great game of power, not clear of it in that way. 

A way i've been thinking about this recently is that there's three different types of strategic actors. There are hyper agents which are individuals that have a lot of agency that extends through them. Through being in a controlling position of a major nation state like xi jinping's relationship to china or whatever, or through control of their own corporate endeavors. Whether we're talking about Bezos or Musks or anyone who has you know massive kind of enterprise capability. 

So we have hyper agents, we have institutions which are a different type of strategic actor. We can think of the united states as an institution. Then of course we can think about sub-institutions within it the CIA is obviously an institution. The united nations council foreign relations, those are institutions that have significant capability of different types that don't depend on one particular hyperagent they may or may not have a exceptional hyper agent at the helm. They will often be the result of things that have already gone through cycles of succession, and where there is some kind of law or operating agreement or a kind of codified structure for how the coordination of everybody in that thing operates together. But where collectively acting as an agent it is something that is a major player in the great game of power.

Obviously you have some institutions that are subsidiary to other ones in the same way that you have some hyper agents that are subsidiary to others. The question of which ones are not subsidiary they are kind of tier one. That doesn't mean they're totally free because they're still bound by multi-polar traps, and they're still bound by what their own constituencies require.

A good friend and colleague of mine Samuel Borya asked the question in a conversation with me once of how many countries are there actually? If you think about a country from the point of view that has real national sovereignty to kind of do the things it wants as opposed to pretty much has to toe the line of what other more leading countries or coalitions require of it. There's a question of is it only countries that have nukes? but it's definitely not a hundred and ninety something countries that meet that criteria. It's maybe like ten or something. It's a much smaller number. 

## Egregores

So you've got hyper agents which are individuals that may run an institution, may have some role in a number of institutions. You have institutions that may or may not involve different types of hyper agents, but are ultimately not hyperagents centric. And you have Egregores. Egregore is a term that has kind of come into popular use recently that was not in popular use for a long time. I believe the origin is maybe medieval magic. The idea that a group of people could be engaged in some type of coven or collective ritual that could create a spirit of that group that was different than just the individual people. 

The way it's been applied recently is the idea that there are kind of self-organizing mimetic tribes or cultural groups that end up acting with some kind of coordinated capacity, but without being institutions that don't have like charters or mandates for how they operate that are not run by a hyper agent. These are really kind of emergent cultural phenomena that still have massive influence. While there are lots of smaller ones if we look at the western world in particular focus on the united states right now, people can have a sense of something that you could call wokism and something that you could call Magaism or something like that as kind of two competing Egregores that both have pretty huge percentages of the population engaging in some way. 

Of course there are sub agricores within that that might even be competing with each other but also coordinating at scale for certain types of things and it's interesting because if you think of like wokism as an Egregore has it influenced many institutions? totally. Has it influenced many hyper agents? and does it work to either decrease the power of hyperagents to kind of go against what it values or increase the ones that seem to support what it values totally.

Then when you look at the counter response of say kind of anti-wokism does it influence institutions and influence hyperagents. So you can think of Egregores were kind of emergent cultural attractors as another strategic actor and then you can of course see that all three of these types of strategic actors inter-affect each other.

A particular hyper agent might try to protect itself from the attacks of a Egregore that doesn't like it. It might try to even spin up the Egregore that supports it more to attack the other competing Egregore. It might do this through being good at social media, it might try through lobbying or campaigning or whatever it is to influence institutions in its favor for tax credits or subsidies or government contracts or whatever. An Egregore might try to influence an institution, you can see they all would be inter-affecting and they all have certain types of multi-polar trap type dynamics. So we can see it's interesting.

Something about hyper agents that is not an absolute law but is a generality is that the people who are most oriented to attain lots of power whether it's market power, political power, or cultural power. The people that are both most oriented to do that and good at it dispositionally and the people who would be the best stewards of power from the point of view of collective well-being dispositionally, are generally almost mutually exclusive dispositions.

They're very different dispositions the people that would be oriented to want to get rid of asymmetries of power and create more kind of symmetry of empowerment of everyone and factor everybody's perspective. You might consider who you would want to be in such positions of power are much less likely to get or maintain those. So this is something that the world kind of needs to work on that is interesting is how do we develop people who have some hyper agency capacities but that are more aligned with collective and environmental kind of well-being, and who don't lose the capacity to have influence because of the care of those other considerations.

Now it's also true that while institutions often steward much more total resource than any hyper agent, they have coordination challenges because there's lots of people having to coordinate. So they're often at times slower and particularly institutions are usually founded by hyper agents or particularly effective people and then oftentimes become more bureaucratic and they undergo institutional decay of different kinds ways that [Baudrillard
or others would describe how institutional decay occurs](https://baudrillardstudies.ubishops.ca/effervescence-and-implosion-in-the-sociologies-of-emile-durkheim-and-jean-baudrillard-towards-a-sociology-of-religion-at-the-end-of-the-social/) . So after a while a hyper agent can often outmaneuver an institution which also is a good example of why it's very hard for states to regulate say new emerging tech companies because the tech companies oftentimes still run by a founding effective hyper agent are just going to be able to legally and otherwise maneuver faster more effectively. Especially as the state is undergoing increasing decay as a result of things like social media that polarize the population that elect a polarized representative class that can't agree on anything

Then you of course have a situation where the exponential tech is both increasing the coordination capacity of hyper agents that run top-down or organizations while decreasing the capacity for coherent action of democracies that are now suffering from the polarization that it creates. 

So we can think about how would we make institutions and then there are lots of types of institutions that are just intrinsically sociopathic. If the institution has to be measured by continuous growth it has an embedded growth obligation say. We're talking about a corporation and the nature of the fiduciary responsibility to maximize profit returns to shareholders. The nature of limited liability is such that this entity can't not be sociopathic. 

It's just the nature of the thing that people who are in positions of directorship would actually be in violation of the law of fiduciary responsibility if they didn't maximize profit even if that means externalizing lots of harm. Like we actually created institutions that have massive power that are obligately sociopathic that has to change. So how do we design institutions or not obligately sociopathic that are the best designed in terms of the effects of that power. Then how do we also protect them from institutional decay so that as time goes on they actually become more coherent and effective rather than otherwise, so that they aren't susceptible to capture and corruption.

## Technology and ethics

How would we support the cultural emergence of things like Egregores that are not oriented against an out group but are oriented in some way other than against an out group? How would we bind effectiveness at power with wisdom and good stewardship of power, in the power that individual agents or hyper agents get? T

These are all the types of questions that are important when we think about how do we orient towards a third attractor that is not dystopia or catastrophe thinking about it in the through the lens of the great game of power, and the nature of the players at play currently?

We were talking earlier about how there would not be human-induced catastrophic advanced technology if there wasn't at least industrial level technology. There wouldn't be the risks if there was not environmental risks at scale and obviously nuclear and exponential technology for the more rapid kinds of risks that are possible. So there's something about the power of human choice as increasingly leveraged not just through the number of people, but way more powerfully leveraged through the amount of tech that is key to how bad our bad decisions can be for catastrophic risk.

There's something about understanding technology that's obviously fundamental and we were talking about how about how artificial intelligence, as maybe we can say that computation is the kind of center of exponential technology. Computation makes processing complexity at way more scale possible and computation allows for the development more advanced computation which then can be applied to the development of every other category of tech.

So computation being at the center of exponential technologies and ai obviously being at the center of computation in terms of self-learning capacities running on computational substrates. We talked about how there are some of the nearest term fastest moving most challenging existential threats related to artificial intelligence. How certain types of artificial intelligence could play key roles to being able to develop technologically augmented human collective intelligence systems capable of solving multi-polar traps and creating a world that is aligned with this third attractor neither dystopia nor catastrophe.

It would seem like maybe we're saying ai could be really bad could be really good depends on how we use it and that would align with an idea that is pretty common today that is technology is not good or bad it is just neutral. It's a extension of human capacity and the value systems that humans bring to it are good or bad or whatever, hey're motivated by whatever other things they have whatever ethics we might want to consider. But that tech is fundamentally values neutral.

I'm bringing this concept up because it's a common concept and i think fundamentally and critically not true in a way that because of the role tech plays in catastrophic risk we have to understand this more deeply. There's a view of technology that actually says technology is fundamentally good we can use it for a negative application but fundamentally it is good because we're inventing technology to solve problems. We wouldn't invent it if it didn't solve some real problem that we care about and then it wouldn't get selected for the market.

Market forces wouldn't upregulate it if lots of people didn't appreciate the way that it solved that problem. So intrinsically technology equals problem solving and extension of human agency and capacity equals fundamentally good. That's kind of like a simplified core idea of modernity, and then everywhere where that wasn't true and of course we can see lots of ways that that was true. But everywhere that wasn't true, the social critique of it came up in in post-modernism in particular and there's kind of a luddite interpretation that technology is more kind of fundamentally bad. That it will advance some people who are good at using it at the expense of other people and in inexorably since it is power it will create more asymmetries of power and abuses of power. 

It is good at converting the natural world into human resources which also means the destruction and colonization of the natural world.

As good as we might consider the [Haber-Bosch method and nitrogen fertilizer](https://en.wikipedia.org/wiki/Haber_process) from the human point of view of how many more humans grew, we can also say none of the environmental destruction in the world that it exists at scale would have happened without being able to go from half a billion people to eight billion people. This vast increase in population use  radically more resource per capita, and then dumping all that nitrogen in the ocean then and so there's this view that like tech is increasing our ability to mess things up, disconnecting us from nature disconnecting us from each other creating a world at a kind of scale that nobody can actually comprehend or take responsibility of.That back to nature, back to simplicity direction would be better.

So the idea is that here tech does have an effect on human values it's just fundamentally bad, i would say that both of these views that it's fundamentally good and fundamentally bad or the other one that it's fundamentally neutral i would say all of these are insufficient. The view that technology is the luddite direction that it's in inherently negative it's not true. There are technologies that predispose changes in human behavior, that move in the direction of more consideration of each other, more intelligence more advances of culture that most people would think of as positive things.

If we talk about as we mentioned before the printing press giving the capacity for a newspaper so everybody could be informed, and the capacity for textbooks so everybody could learn and increased the access to knowledge and informedness and participation decision making and say well that technology had absolute cultural effects that were aligned with directions that we appreciate. I would say that it's important to get that technology is values affecting - it does affect human values because it affects human behavior and human behavior affects human values and in ways that are complex that can be positive and negative and different metrics simultaneously.

## Technology changes society

A classic example of this in [sex, ecology and spirituality by ken wilber](https://www.goodreads.com/book/show/177151.Sex_Ecology_Spirituality) that got me thinking about this many many years ago. This was the idea that the emergence of the plow as a technology radically changed religions and spiritual ideals. It also changed economics and cultures and cultural institutions, meaning it affected human values as a result of the technology itself. 

To just kind of play this example out and i'm sure um the historians would debate how this occurred differently in different parts of the world. So this is very broad brushstroke, the plow the kind of ox or horse-drawn plow does provide the ability to get a lot more caloric surplus out of an area than just a human using a digging stick. Being able to have a much bigger tilling or digging stick behind animal power is going to increase food production. As a result it's going to allow the population that uses it to grow their population and make it through famines better than the one who doesn't use it. 

So the technology proliferates because we were saying in the positive example it solves problems but we can also say it increases game theoretic capacity, increases power relative to a group that doesn't use it. The first insight there is that new technology that confers power so the first insight there is that new technology that confers power becomes obligate to use or use any culture that doesn't ends up either going away or becoming a vassal culture to whoever does use it.

That's a really key insight is that the utilization of new more powerful technology is not a choice it is obligated if you want to continue to have any sovereignty. Because of multi-polar traps and then that because that technology confers power to those who utilize the technology will get that those in ways that those who don't don't. Utilizing it means a different pattern of behavior - now i'm behind a an ox or a or a horse holding a plow and beating the ox all day, as opposed to being out hunting or something else. 

So it's a fundamentally different pattern of human behavior to utilize that technology, and the fundamentally different pattern of human behavior also ends up conditioning the nature of human experience minds values etc. If you think about this example the assessment i had read here said that following the implementation of the animal drawn plow across all cultures that implemented it - animism decreased within a fairly short period of time something like within 100 years. The cultures before the plow were all animistic and afterwards were not animistic. 

This is a humongous change to the ontology and spirituality and value systems and experience of people. If you think about that across pretty much every continent everywhere. There are people they were animistic before that meaning the spirit of the trees and the animals. If i'm a hunter-gatherer i can't kill an animal in a hunt and still believe in the spirit of it and can do it with reverence and maybe not kill pregnant mothers or when the herd is too thin. It is maybe i can kill it make a prayer maybe cry while i do, it say okay i'm killing this buffalo to feed my kids and then when i die i'll go into the soil to create grass to feed your kids. The great cycle of life, but still be animistic.

But can i yoke an animal, so can i take that buffalo breed it into an ox yoke it and beat it all day long and still believe in the spirit of the ox? not really right and i have to now be like ah it's just a dumb ox man's dominion over nature. We were put here to have dominion or to control or exploit or they were put here for us. It's a totally different value system that's going to occur when we move into a kind of animal husbandry where the control of that animal's entire life against its will now is what gets us ahead as opposed to some kind of free symbian relationship. 

So then if it's just a dumb ox it doesn't really have feelings does that bleed over to the way we think about other nature well yeah because now we're also going to clear cut whole areas to turn them into areas to plow. So now the beginning of the anthropocene in a way you could say the where nature becoming a productive space for humans is what is useful, rather than nature for its own purposes. Then a plow took meaningful upper body strength to be able to operate so instead of previous situations where either the men hunted and the women gathered or the men hunted and the women did horticulture with a digging stick - now the men were hunting or doing the animal husbandry for meat but also now producing the vegetative stuff.

So there was evidence that not only did animism go away, but the distribution of male and female gods before the plow was pretty equal in most areas and then it moved to increasingly more male gods with agricultural mythos, agrarian mythos rather than hunter-gatherer mythos. Because where both men and women were providing food stuff and calorie and support for life, now just men were providing it. That was reflected in the nature of the mythos so is some of what we think of as like the patriarchy even the emergence of just a tool right like the plow, but then the fact that it provided enough dominance capability that it became obligate and using it created patterns of behavior that again created whole patterns of mind.

Then that created way more surplus than it ever been, there was surplus before but not as much as when you could row crop grain and then be able to store huge amounts of calorie for a longer period. Which both led to population booms and people depending upon their own surplus more than depending upon their connection with cycles of nature. Then exchange increased and all the things that we think of as because of the exchange and being able to use grain as a medium of exchange, you got increased specialization and division of labor and like that.

But also the emergence of class systems and radical kind of wealth inequality and then of course as more there's more surplus and there's private ownership of that surplus. Then there's inheritance and then with inheritance becomes the need for things like institutional monogamy for paternity certainty to know who the inherent attendance passes to. If we start to think about things like private property ownership at scale and class systems and institutionalizing mating structures and parenting dynamics and inheritance, and animism and religious ideas and the anthropocene like huge effects on human psychology and values at scale that have been determining features of the world that were the result of a technology or at least influenced by technology.

You get like okay technology is not just values neutral where it can be either good or bad and it just depends on how humans use it we can grow grain to feed our people versus grow grain to support a military. It's not just that it's the use of the technology itself is affecting the patterns of human behavior in ways that are affecting the psyche in ways that at scale affect the society and the culture at large. So it's not just that we build a technology that benefits one thing physically that might also harm other things physically which is what we think of as externalities physical externalities. 

It's that it's also has effects on human psyches and cultures some of which are positive and some of which might be depending upon the ethical framework we're looking at quite negative. So there are externalities mediated both physically and psycho psychosocially when we get that tech can't not influence human psyches. Then we realize that it's not just how are we developing goodness or badness, how are we developing ethics and wisdom or whatever in people culturally, so they use tech properly. 

It's also how is it being coded into the tech where the use of it in turn is affecting the nature of the values. There becomes this recursive relationship and so we're referring to this as axiological design. Axiology being kind of like ethics and aesthetics the things that are other than just science, the cultural elements - how do we how do.

We think about the way that any particular technology is going to affect the patterns of behavior of those who use it because of the advantage it confers as a result affect the nature of minds, as a result affect the nature of cultures, and how do we think about the externalities created there. So we factor that into the design so that we are designing technology that is promoting human psychosocial flourishing and wisdom rather than the other way around.

## Infrastructure, social structure, and superstructure

This brings up a a model that i think is important for thinking about the metacrisis, catastrophes dystopias and a third attractor which is the model of thinking about civilizations in terms of the infrastructure the physical technological infrastructure that the civilization runs on. Including the social structures, the kind of agreement fields of human action that mediate engagement, and the superstructure or the kind of ordinating values of the culture is oriented towards the societies were entered towards.

Those three terms infrastructure social structure superstructure i believe were first developed or at least popularized by a anthropologist named [marvin harris a book called cultural materialism](https://en.wikipedia.org/wiki/Cultural_materialism_(anthropology)) came out of marx and other people's kind of analysis of looking for different types of societies or civilizations. How do we think about the major components of what makes them up and what makes them work the way they do.

So this triplicate model says the the infrastructure is basically how the people meet their physical needs, and relationship with the physical world mediated through their tech stack and so that's energy generation agriculture water mode of transportation, manufacturing waste management all that kind of stuff. We can see that historically from early tribes evolution was not selecting for people without their tools, evolution was selecting for and it wasn't selecting for individual people it was selecting for groups of people with their coordination capacities and their tools and not individual tools but their entire tech suite.

This is actually such an important thing to get is that again why are we very different than apex predators. Some earlier hominids started to develop capacity for increasing abstraction that made them more oriented to increased coordination capacities like language mediated things and increased not just tool use but tool making and tool advancing capacities. Maybe starting with like homo habilis or something like that and then obviously getting doubled down and homo sapien who then probably played some role in the extinction of the other hominins.

But now we can say hope sapiens versus everybody else that we see other animals will utilize tools, they are not evolving tools and technologies in a similar way. They're not evolving coordination systems at scale in a similar way, those are mediated by the capacity for abstraction. Where rather than just use a rock in a way that a chimpanzee will use a rock to cut something experiencing the sharpness of it and they will then pick up a different rock. They can feel which one cuts faster and they'll use the one that cuts faster but the principle of sharpness that all of these rocks have in common is an abstract principle that is different than any rock.

It's not the instantiated rock - it's a principle to say how would i make something with more of the principle of sharpness to then start you know chipping flint or something like that. The capacity to understand sharpness or leverage to then be able to make something that has more of that and continue to advance it that abstraction-mediated capacity probably requires enough neurophysiological development in prefrontal cortex and overall brain.

We're just on the other side of having that capacity that same capacity for abstraction also allows us to take the reality that we experience and symbolically mediate it. For example, through language in different ways that allows the ability to take all of what we've learned in the past and do things like compress it into oral traditions and compress it into writing to be able to start to have cumulative knowledge that moves a lot faster.

We can say that evolution is kind of determined by genetics, but that with sapiens genetics came to a point that memetics became now the selective dynamics that our hardware got to the capacity that it could evolve its own capacity without hardware changes just with software changes. So then the software updates move much much faster so obviously humans today are genetically not very different than humans 10,000 years ago but human capacity is radically different not because of genetic chains but memetic change.

So that means both change in our ability to understand abstraction and create more powerful tools and more powerful tech stack, and more powerful coordination protocols. So the difference between humans today and 10,000 years ago is quite evident in our governance and economies and coordination protocols and our technology stacks, and not in our brains or genes.

This comes back to again the power of gods, and the love and wisdom of gods that is needed to bind it and get to that in a moment. So individual homo sapiens in most early any early environment would not make it very well. It was groups of people in tribes as social primates that made it, and groups of people without technologies would not do that well. The capacity to make spears like that was really important for evolutionary advantage. So it was and it wasn't just individual technologies it was suites of interconnected technologies.

Obviously the moment you make a computer you also need a monitor and a keyboard and a mouse and then an internet and a lot of other things to make it useful. So technologies don't emerge on their own, they emerge in ecologies of technologies that advance themselves. So if i have the ability to harvest more grain because of a plow i better also get baskets to be able to do grain storage or it's not going to be that useful. 

So you can see suites of technologies that create effects and emerge, so what has been selected for is groups of people and their coordination protocols with their tech stacks or in other words what marvin harris would call their infrastructure. Which is all their technologies their social structures, which is their coordination protocols and their super structures the culture that kind of both binds them together and orients what they are working towards.

You can kind of understand any cultural civilization in terms of the intersection of those three phenomena. There are other models we could use but i think this is a useful one. What i'm trying to get across here is not this particular model or what marvin harris had to say about it - but the idea that when we're thinking about the world as a whole right now, we have to think about the various aspects of it that inter-affect each other to think about what has to change in each of those aspects, rather than look for like what is the problem in a reductionist way.

I was saying infrastructures are kind of tech stack social structure is the kind of institutions and agreement fields that mediate say governance law. Economics can kind of fit into the intersection of social structure and infrastructure. There are super structures like religion spirituality sometimes, patriotism sometimes ethnic or other types of cultural forces. 

But basically the collective or the shared values that determine what is the good life what is good and bad what are the ethics that become the basis of jurisprudence that law then gets based upon those types of things. When you realize that our value systems or superstructure are influenced by the by the infrastructure you can see that religious people have paid more attention often to the effect that tech has on values because they see it right.

Which is why things like the sabbath let's have at least a day, where we're not using tech and there are other things to it not. But not using tech not doing productively oriented things and are kind of only superstructure focused or the reason that amish or other religions that want to keep certain traditional values reject tech that would otherwise change it. This is the recognition that the tech will change it.

I would say ultimately we have to have a value system that can be or a superstructure that can be evolving. Where the dialectic of both what was wise in it that emerged historically which is traditionalism and what more can still emerge that is important for changing environments progressivism that dialectic is well navigated in a superstructure that both maintains and resists change that would be of an entropic kind. While embracing the chains that are truly kind of critical and important of a syntropic kind.

So it is both true that infrastructure influences superstructure. It's also true that superstructure influences infrastructure. The people that really value a particular thing can go innovate and work towards the development of that thing. People that value a thing can create law that binds the use of that thing or that subsidizes it to make it happen in a way that the market wouldn't or taxes it to happen less. Marvin Harris argued the way heavily. 

The way that infrastructure changes social structure and superstructure which is very important. Other social theorists have talked about actually the social structure's most fundamental, because whatever the market incentivizes is kind of what wins. Or then someone who had more kind of state orientation says no actually law can bind things that even if the market would incentivize just aren't allowed to because of a monopoly of force.

The power really rests within the social structure, the state to be able to bind things and other people talk about why the binding power there only happens if the people are checking the state and are developing their values in a way that becomes the basis of the law. So superstructure is really the key i would say they're all inter-affecting and rather than consider any of them fundamental or epiphenomena i would say that they are all fundamental inter-affecting. 

To design a civilization that meets this third attractor it has to be a infrastructure a social structure and a superstructure that all have the right criteria in relationship with each other. That means that the kinds of interventions that we're looking for the changes that we're looking for will be in all of those areas you know and we often hear people talk about. The problems of the world come down to problems of mindset and paradigm and there's some kind of cultural awakening, some kind of evolution of our values evolution of our sense making evolution of our shared identity that needs to happen.

Some people talk about, well we actually ultimately all these problems have perverse economic incentive, and the fact that the embedded growth obligation of the financial system is incommensurable with the physical world. We have to change the economic system to change anything, and that if you remove perverse incentive everything would or in the right direction. Others might be arguing we have to create technological solutions because it's the only thing that can implement at scale quickly enough that solved the world problems.

I would say none of those answers are sufficient, and they're all important, and what will actually make it through is not a theory of change but an ecology of theory of changes. Like multiple different solutions that have virtuous relationships with each other that in if i make a technology for improving energy efficiency then we get the [jeven's paradox where an increase in energy efficiency means we use more energy not less](https://en.wikipedia.org/wiki/Jevons_paradox) because now energy is more efficient becomes cheaper.

As a result, a whole bunch of areas that were not market profitable now become profitable as energy as one of the sources of input becomes cheaper. Now you know a bunch of mirrors become profitable and so whole new markets open up. We end up using more net energy so in order to actually fix something like planetary boundaries we would need both new energy generation processes plus energy efficiency processes in tech. But also multi-polar trap bindings kind of in law that kept us from then saying oh well let's just keep using more energy.

How do we actually create a different economic system that doesn't have an embedded growth obligation, that gets very deep to even like interest, and how do we create law that can bind the development of new market areas that are incommensurable with the biosphere continuing to happen. Then simultaneously that's going to require some cultural shifts of people that are aligned with that and want that so it doesn't feel imposed.

So i don't think that we find adequate solutions in any of those areas - we find adequate solutions between those areas contemplated together.

## Wisdom of Gods

So we've talked about the drivers of many of the historical challenges in the world that at current scale portend catastrophic risks. 
We’ve talked about the drivers of the metacrisis as being related to collective action problems. Whereby our ability to develop and implement increasingly powerful technologies exceeds our ability to implement adequately responsible governance of that power. 

Talking about it mythopoetically as power that is more than apex predator's power, that if we wanted to think about what to compare it to in literature would be like power of gods and i don't mean capital g but that some commensurate type of wisdom responsibility prudence is necessary to direct and bind that power. If that is a reasonable framing of some of the problem space then it's a reasonable question of is that possible or are there problems that are tied to human nature inextricably.

Certainly many people have asked that question and come to that conclusion, and then either come to the conclusion that because of that civilization will probably self-terminate and maybe one of the reasons we don't see more aliens is because maybe the fermi paradoxes are because of a fermi's gate. Where most civilizations when they get to their place of technological adolescents blow themselves up, that's certainly one kind of narrative trope. Or that humans are genetically too rivalrous and irrational and it really is a genetic human nature thing. So that maybe if we were to make it, that requires the genetic engineering of the human to be a genetically smarter and less rivalrous creature which is certainly also a certain type of sci-fi proposition.

Or it requires something other than the human primarily guiding things like super benevolent super intelligence ai singleton that can run things for us that won't have the rivalrousness and irrationality and collective action problems that we have. Which is strangely a lot like in certain ways memetically like waiting for the return of the savior or the the next phase of the mayan calendar or some type of thing where omething with closer to parental type capacity is fundamentally different than us will rule us, because we are too to rule ourselves. The historical assessment of technology possessing empires using their technology in maximally responsible and omni-benevolent ways both to in terms of like inequality issues within the society and then rivalry issues between societies and in relationship to nature like our our track record's not that good.

We have not been the most omni-benevolent stewards of our power and this is why dystopian sci-fi is so much more easy than protopin sci-fi is because imagining creatures like humans like we have been historically with exponential power. It’s real easy to imagine how we mess everything up, there's lots of ways to do that. The idea of a world where we have that much power and don't mess everything up and don't solve the issue through some type of totalizing control mechanism that becomes dystopic like the borg or whatever. It's actually hard to think about that, and you know sometimes you go avatar like some spiritual enlightenment must occur where we have the tech and like telepathically all communicate about how to use it properly, and don't use it wrongly. But it's like okay, there's some and then magic happens. 

So it's a fair question are humans innately better at the types of intelligence that develop technological power than they are at the types of intelligence or aesthetics or ethics or whatever you want to call it that - that would have a steward that well at scale in the presence of other people doing the other things that they do in things like multipolar traps. I don't think there is anything fundamentally impossible about the task in terms of the genetics of human nature. I don't think that the goal of individual and collective wisdom adequate to steward the power is an unattainable goal, or that it requires brain computer interface an ai overlord or genetic engineering or magical events.

The many reasons why i would say that in the debate of all the counter-arguments is again beyond the scope of this, but i'll give a couple examples. In terms of this real politic assessment of our humans too irrational and rivalrous. First, when we gave the example earlier of lead taking a billion IQ points off the planet, and leaded gasoline and increasing violence by 4x or whatever it was. That's a pretty significant change that is not based on human nature, but is affecting human physiology ubiquitously in both of those metrics and there are lots more and of the whatever 50 million chemicals that are in the chemical database. How many of them have effects on cognition and psychology and behavioral motivation? how many of the endocrine disruptors and neurotoxins and xenoestrogens and the environment that were produced externalities as a result
of the better living through chemistry mode are affecting us in these ways. 

I think there is a humongous amount of what we are perceiving about human behavior that is physiologically mediated a lot, and then also mimetically and culturally mediated and i think the exploration of the full phase space of what could happen to both attenuate the negative contributors and advance all of the different positive contributors and what the potential of the whole of that space is is really profound. If we think about things even like watching how quickly a technology like social media like facebook can increase people's enmity towards their fellow citizens of the same country to the point that both sides are are oriented towards war. So rapidly can vitriol and anger and would ever get stirred up as a result of just what the media that people are being exposed to, so much stuff. What is being concentrated by the ai that's selecting for me could that same technology curating with a different set of purposes that was bias correcting rather than doubling down on bias that was helping people connect to much wider networks rather than tribal networks, that was helping to up regulate more synthesizing and nuanced responses, rather than other ones like could that affect culture and intelligence and disposition at scale in a way that no religious prophet could have ever dreamed of historically.

I think the potential there is so scary and also so fascinating depending upon how it's utilized, and when we look back historically do we see that all cultures were kind of equally irrational and equally violent and across all times and the answer is totally not right. So then we say independent of what probably has nothing to do with genetics and human nature in that way were the cultures right. When we talk about infrastructure social structure and superstructure, all of that is with the same human nature - and yet you can see radically different human experience and patterns of human behavior, and different superstructures. So do we see that there were some cultures like judaism that invested in education across the whole culture much more widely than many of the adjacent cultures to them in the embedding environments.

So whether we're talking about what a high percentage of people are nobel prize winners that are jews compared to their relative percentage of the total population or how much higher the median level of education is of the jewish population or how much higher the lowest level of education of one standard deviation is in the general population . It’s like the entire bell curve is highly displaced as a result of a cultural phenomena. So can culture affect that at scale? and do we also see situations where you have much less defection you know within the say jewish community on each other because of collective intelligence systems structured into the way that religion and culture works?

When you look at cultures like buddhism or even more significantly like jainism and to some degree quakerism, do you see cultures where across the whole culture you have the bell curve of violence radically displaced? where the median of violence plus the positive and standard deviations are all way different than the embedding culture that they're part of? totally. So how violent or rivalrous people are, and again if we're talking about buddhist culture or jain culture we're talking about not getting to hand select for the right people genetically. We're talking about across the entire gaussian distribution of genetics that the entire distribution of behavior on these things is really fundamentally different. 

We can also see that there are some cultures that orient towards anti-intellectualism, and have value systems around that and that orient towards violence and rivalry. So you can see that if we're talking about a small subculture like the janjaweed, where you know the the child soldiers in darfur Liberia that to make it to be part of that culture in adulthood probably the whole population has to demonstrate sociopathy to make it to adulthood. Whereas you might have other cultures where there's almost no sociopathy in adulthood. So i think we when we do sociology, we study patterns of human behavior under ubiquitous conditioning without factoring that it's ubiquitous conditioning and without factoring that one of the things that is so unique to humans is how plastic we are to be radically different based on the environment we're in. 

This is really a novel aspect of sapiens, again why we don't fit the apex predator model is because the the polar bear doesn't go become an apex predator in the savannah right? and the cheetah doesn't go become an apex predator in the arctic. We went and became apex predators everywhere, and we can out fish the orcas in the ocean we can out hunt because our ability to create technologies like homes and clothes and whatever allowed us to go be apex predator everywhere. Because we move environments and because we change our environment so much. Whether what's adaptive for me is learning how to text well or learning how to throw spears is really different - or speak mandarin versus speak english. Humans can't be born with mostly kind of genetic programs for how to operate in the same environment with the same tech stack that they've been forever the way that almost every other animal has.

We are born into an environment where we have to learn whatever the new language is whatever the new environment whatever the new tech stack. Which is also partly why  a human baby is so incapable for such a long period of time, because they are having to encode what is effective and adaptive in this environment because we change our environment so much and evolution is the process by which animals become fit to their environment. We then in turn change our environment so radically that we have to be plastically upgradable to fit that environment.

I would say that the janjaweed and the jains are both within human nature, and of course historically the the most violent cultures didn't make it through. Because oftentimes if they were violent towards each other they couldn't coordinate to do anything at scale in the same way that a bully usually doesn't become a tier one hyper agent. It’s not simply their rivalrousness. But their capacity to do it effectively at scale so the cultures that make it through are not the most violent. They're the ones that can do violence effectively at scale which also means that they have to do a lot of very positive stuff within the culture effectively like specialization and division of labor and coordination and things like that.

Robert Wright wrote a book [Non-zero](https://www.goodreads.com/book/show/9526993-nonzero) that was kind of arguing that what evolution is selected for is increasingly positive. Some dynamics that is part of the truth but it's been the intersection of the positive sum dynamics and the zero-sum dynamics and the winner-take-all dynamics that defines why you both have increase in collective intelligence of certain kinds and increase in rivalry of certain kinds. So those perspectives have to be factored together but historically there was a radically different distribution in how rivalrous, and how rational cultures could be. But unfortunately the selection criteria have multipolar traps that are not aligned with what we need them to be long-term. 

But can we create a situation in which something that is less rivalrous internally and more good at sense making values generation and choice making also can protect itself and proliferate. Where if it were to proliferate totally it would actually be a sustainable and not just sustainable but regenerative and evolving good culture. I believe that is possible in a way that wasn't possible previously because of the tech that can mediate that type of coordination. But i think when you take the positive outliers of cultures and then the positive outliers of individual humans on both scales, we can see humans that individuals that look very different in the right direction and whole cultures. Which means that it's not just the result of unique weirdnesses genetically of those people.

## Education

Another thing that i find kind of inspiring here is there's an article i came across a while ago about why the aristocratic tutoring model of education, and the hypothesis that throughout much of history and particularly modern history what we think of as the greatest polymaths, the greatest super geniuses what did they have in common that's a very interesting topic of educational theory. How much of that is just some innate thing that can't be replicated, versus how much could be replicated. Some super geniuses themselves trying to understand themselves did this study like charles sanders purse. But one of the things that has been widely attributed is that the people that had the most kind of deep insightful genius across the most topics, were the result of an aristocratic tutoring model where they had the very best tutors early because of the wealth dynamics that could do that. 

If you think about meditations by Marcus Aurelius. Marcus Aurelius was maybe about as close as the world knows to a philosopher emperor. The whole first chapter all he does is give credit to all of his tutors, and if you think about growing up to being conditioned to be an emperor you have a scenario where the very best mathematicians and logicians and rhetoricians and historians and whatever are all your tutors. That's kind of their patronage gig. 

Of course that kid is going to grow up differently if they get to have customized education unique to them of the very best thinkers, and there was another study that i saw that was looking at world-class mathematicians and what were the outliers that were there any statistic commonalities of people who became world-class mathematicians. It found that the most obvious indicator was that the world-class mathematicians almost all studied with a world-class mathematician while they were young, because can you become a world-class mathematician from people who are average mathematicians. Can you learn to think that way and feel and be inspired that way? probably not right like it's the exposure not just to the topic but to the generator function of the patterns of thought and the patterns of perception that would invoke that in somebody.

Well we have a world where the people who become the most competent and they think almost never get to become teachers, and if they do it's maybe a few classes at an elite university maybe grad school that very rarely get to teach kids. So how how could the kids get that? We can see why the aristocratic tutoring model could have made that possible for a very small number of people, but was radically undemocratizable. We kind of moved to a more democratic system and wanted to make education available to everybody and also saw the radical inequality of that as kind of a terrible thing we want to get rid of.

There are also less people being developed that would have the kind of deep and wide synthesis capabilities to handle the complexity of the issues in the world. So there there's a question of some of the analysis looks at the aristocratic tutoring of many of the great thinkers that everybody knows from Isaac Newton, Descartes whatever that up until fairly recently that like Einstein and Von Neumann had mathematician governesses shows that even were young that had already conditioned their patterns of thinking by the time they went to school. 

Is there a way to democratize or to like make that type of capacity widely available? because we could imagine we could condition much much higher potential realization and of course it doesn't have to just be in math and science but in arts and philosophy or humanistic and connection and ecological endeavors. But where the people that have a genius in the thing are actually spending deep time with and the tutoring and the development of new humans. I get so excited thinking about the possibility. 

A couple things come up when we think about both robotic automation and ai automation that is emerging that creates the um technological unemployment issue that is one of the huge issues the world faces in the next couple decades. What changes in the world associated with technological automation of most of the jobs and the technological unemployment that portends. There's obviously some really dystopic things where at least feudalism, and there's a way that it looks like the world is going towards a kind of feudalistic attractor right now. Where you have a few people controlling exponential tech and network dynamics with billionaire capacities and the ability to do like stuff that nation states can't do, but with nothing like democratic jurisprudence on what they do which is more like an emergence of a new kind of tech feudalism.

But at least in feudalism there was a need for all the people's labor so there was some, and the tech asymmetry between the ruling class and everyone else was not so much that if the ruling class was so evil the people wouldn't be willing to die with some pitchforks and have a revolution. With this degree of tech asymmetry and a situation where there's not a need for human labor - because the robots can do it there's no need for human intellectual labor as the ai can do it - what happens? So one version is radical population reduction because we just don't need the people to do stuff. Another version is create a universal basic income, so since there's no real jobs for people at least don't rebel too much and maybe they will spend most of their time in the metaverse - so that they can have all of the things that they want where it doesn't actually cost anything.The fact that they are basically serfs is fine.

There's like a number of things like this - there's an idea that maybe there will keep being new market sectors that open up, probably not at the rate that ai does that useful thing. But then you could say, okay well what is it that the ai won't be able to do that is unique to what humans can do and outside of the deepest sense of sentient ai what is the unique thing that humans could do, is things that involve actual sentience actual like subjective experience and connection. I can make an ai chatbot that some does something for a person in terms of connection. But it doesn't actually love them or care about them or understand them in a way that can be felt right? So then could we have a situation where the high touch connection oriented services become where most of human activity goes, and the training to be a nurse or in care in a you know caring position like this gets to have that type of compensation. Though i don't even think it would be pay i think it'd be a different economic system but we'll call it the the equivalent of a half a million dollars a year. So that people's real talent and capacity would want to go into those places and then obviously education being one of the best examples where you could have a much higher percentage of total human GDP oriented to those professions.

You could have a lot more teachers per capita - they get a lot more training and a lot more talented people moving into that. Then of course you also get ai tutoring and when you look at things and a lot of groups are looking at this. My colleague Zack Stein is writing a paper on this right now that i can't wait to read, but when you realize that GPT3 can speak in voices - you can give it all of Thomas Jefferson's or all of Mahatma Gandhi writing and then talk to it. It can speak in that voice, as best as it does kind of a language and theory of mind model.

Do you have a world where you have a metaverse, and you go to the university and you can ask to talk to Socrates or Von Neumann or whoever it is you see a kind of deep fake real-time version of them that is programmed with both their information and the associated fields of information, modernized that they would know and you can dialogue with them and have that kind of aristocratic tutoring personalized to the kid.

So i could maybe go and actually have Von Neumann and Einstein and Kurt Godel all talking about formal logic with me and i'm talking about with them it's like that would be amazing. Nobody not even Marcus Aurelius had access to that kind of thing in the past. Now of course what Von Neumann the deep fake in here says and what actual Von Neumann would say would be different. This is where we want the human tutor to be. 

It very much like robotic surgeons will overtake humans in surgery and largely the doctors will be the interface between a lot of ai capacities and the very high touch human connection capacity. That would be one of the things that the human tutor would be really doing because the human tutor would have a teacher student relationship of mutual trust and love and teacherly authority, and all those types of things and then get to talk to the kid about as the kid just got to learn a ton about formal logic talking to Von Neumann. So all of the content is happening there, but then the teacher gets to say what do you think is different about what that Von Neumann said and what actual Von Neumann might have said based on the theory of mind. Which then gets into understanding the type of artificial intelligence versus thinking about natural intelligence which gets the kids thinking recursively about intelligence itself, and the relationship of of biology and computation and intelligence, and consciousness. As well as the recursion of not only getting to have that ai tutoring and the human interface but the thinking about the difference the recursion on what is intelligence itself, what does that portend for how humans could develop?

Obviously i'm speaking about cognitive parts because that's how we think about education, but i think being able to train with the great philosophers and spiritual teachers of all the traditions is the same type of thing. We can do the same type of thing, so could we have a situation where not only do you have this kind of like ai mediated aristocratic tutoring for everyone but then with the humans that are involved in the development of new humans being able to have a lot more resource allocated to that function. It actually becomes very central rather than mediating all the physical infrastructure stuff. The intergenerational knowledge transmission and human connection actually becomes central to what civilization is if we think about those types of things as possibilities, and we think about the elasticity of what human nature allows for.

## Closing thoughts

Do i think that the type of wisdom both individually and collectively that is necessary to steward the type of power that we're moving into as possible
? So i've been talking for quite a few hours have only covered a per small percentage of the things that i think are important and what i hope makes it through is some sense that the **various problems in the world have shared underlying drivers that as deep and challenging as they may seem are actually tractable**. You can actually think about what the shared drivers are and it moves it from the domain of impossible seeming, to really hard but but maybe tractable and hope that some frameworks for thinking about what is underneath and driving many of the enduring wicked problems and emerging catastrophic risks.

That some frameworks for that are clear, and you can start thinking about those when you look at various problems in the world, and that maybe in addition to the frameworks we shared the idea that there are probably quite a few more underlying characteristics that you can be thinking of. That there might not be categorical solutions but that there are progressively better solutions i hope came across. That the catastrophes regardless of which particular one but the attractor of cascading catastrophes or control mechanisms that prevent it but create dystopias, create a framework for what we want to avoid and. That a third attractor that has the intelligence and regulatory power to avoid the catastrophes, while having checks and balances on itself to be non-dystopic. That the creation of a world system that can do that should be the kind of central innovation goal of the world - of which everything is a element or a subsidiary part. That more total collective intelligence and innovation and motivation being focused on these issues is worthwhile.

I hope that rather than creating a sense of doom or overwhelm, that what it what this information might create is a sense of almost like a validation of something that has been intuited an increased sense of clarity that makes the complexity of the challenge space at least start to feel tractable. Because if it is apprehendable and if there are at least thought experiment-wise possible solutions, then there is a direction of endeavor. To have more people feel that hopefulness that is not the naive hopefulness of not being aware of all these problems, but the hopefulness that comes on the other side of being aware of all of them - and seeing what it would take to move through that and be oriented to that. That's what i hope becomes of this, thank you.
