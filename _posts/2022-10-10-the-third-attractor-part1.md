---
layout: post
title:  "The Third Attractor: Part 1"
categories: [third-attractor]
image: assets/images/lorenz2.png
toc: true
---
In this series of articles we will be exploring the ideas around the ['Third Attractor'](https://blog.livingdatalab.com/categories#third-attractor) - a concept relating to the future of our species and civilisation developed by Daniel Schmachtenberger. A brief introduction to these ideas can be found here.

This article will explore the content of the first podcast laying out Daniel's ideas.

## Introduction

Danel's intention and hope for this work is that it gives a more holistic overview of they key challenges our civlisation faces, and what may be some of the characteristics of possible solutions to these.

As he describes:

>My hope with this content today is that we can give a bit of an overview of some of the
catastrophic risks that the world currently faces, why the solutions to some of those risks can actually make other of the risks worse, and why the solutions to the risk landscape as a whole can create dystopias.
>
>Because to have the power to be able to check all the catastrophic risks can become a very concentrated power that is itself is hard to check. All of these risks are unsolvable without solving the patterns of human behavior that give rise to them. 
>
>But the patterns of human behavior that give rise to them can be named and can be progressively better addressed and that in doing so there is a possibility to create a attractor for the world that is not catastrophes or dystopias, in which we have the responsibility and wisdom adequate individually and collectively to steward the power of the exponential technology that we are creating.
>
>The hope of this as a introduction is that people might feel that it helps them make sense of what feels like lots of different problems in the world in a way that is hopefully clarifying and integrating and they can have more people engage in the kind of innovation and collective intelligence towards solutions to the deepest underlying dynamics.

The rest of this article is closely based on the transcript of the podcast speaking in Daniels voice.

## Sensemaking & The Meta-crisis

What we're going to explore is what I might kind of call shorthand the metacrisis: metacrisis meaning climate change and many different environmental issues from topsoil erosion to dead zones in ocean, to pollinators to biodiversity loss, species extinction etc.
to a lot of new existential risks as a result of new emerging fields of exponential technology AI biotech nanotech drone cyber etc. To the intersection of a economic system that requires continuous exponential growth with a planet that is reaching planetary boundaries that all of these are very serious issues.

Thinking about any of them can seem kind of overwhelming, thinking about all of them together. The only way I have found to think about how we navigate that meaningfully is to start to explore do all of these issues have anything in common? are there any dynamics about how human coordination and collective action and patterns of human choice making and patterns of human choice making intersecting with things like markets and states and technology work?

Are there things in common that both give rise to these kind of human-induced problems and make them challenging to shift and both that collective set of crises and the underlying dynamics that give rise to them? this is what we think of as the *metacrisis* which is I would say is a very a meaningful way to orient to what humanity needs to address at this current phase.

The issue with sense making is a part of that if we can't agree on what the nature of the issues are that need addressed or what good solutions look like - then some percentage of the people go working on a thing they think is important like a particular approach to climate change. The other half of the population that radically disagrees with them does everything they can to prevent that from happening. 

Or since we can't all agree to do a particular thing like nuclear disarmament or not doing the AI weapons or making the cost of carbon higher or to deal with externalities, if we can't all internationally agree to that anyone that consciously chooses to do that disadvantages themselves in a geopolitical game of power relative to others. Therefore nobody can do the effective thing, and then you just have kind of collective action failuresthat create races to the bottom.

If we can't have some better shared sense making of what needs done to inform better coordination, collective choice making obviously - we don't even have like a starting point. So all the conversations on sense making are one really critical part of this and i'm not going to readdress those because we've done that on the Rebel Wisdom podcast people can check it out if they're interested.

I'm going to try to just apply some sense making to look at some frameworks for how to make sense of what's going on that might lead to design criteria for what more adequate solutions could look like, that hopefully get more people engaging in in innovative generative ways along these lines.

## The Two Attractors 

For example, the exponential power of exponential tech makes decentralization catastrophes more likely, as and it doesn't matter which one whether it's this AI scenario or that AI scenario this bioscenario, this environmental scenario or this warfare scenario. We just put them all in the bucket called cascading catastrophes to be able to bind those catastrophes.

It has to bind all of them, because any of the catastrophes are enough to break the thing down to bind all of them requires really centralized power, or one possibility is really centralized power i'm going to say that there's another possibility. If it goes decentralized power capacity then that ends up looking like dystopias - those are the two attractors that civilization is currently most likely heading in.

We can see how the solutions to either one orient to the other one more if you try to say we don't want these dystopic things so we want to stop authoritarian tech, we want to decentralize and democratize technological capabilities then we get more collective action failures and decentralized catastrophic tech. If you're like we don't want this, we want some surveillance on this and whatever then we get more dystopic tech.

So I would say that we are seeking a third attractor that is neither catastrophes or dystopias seems like a pretty fair starting place for how do we design a civilization that is desirable, or at least allows us to continue to advance on the meaningful questions.
For example, what a desirable civilization involves is being able to avoid the twin attractors of catastrophes and dystopias and that those two must always be taken in consideration.

Because it's very easy for a partial solution to externalize a problem somewhere else. As we've mentioned before that one of the major problems with the world is the problem with our problem solving we define the problem generally externalize harm somewhere else. That these we can solve for one catastrophic risk while making another one more likely. We can also solve for catastrophic risk as a whole making dystopias more likely.

So we want to be able to hold the undesirable possibilities that are in relationship to each other at the same time we want to be able to hold all the problems at the same time, to then be able to say what does a solution look like it decreases the likelihood for all of those simultaneously.

Since such a third attractor is defined at least to start as being something that avoids the other two attractors of catastrophes or dystopias - we're going to start by sketching out what the catastrophic risk landscape and the generative dynamics that give rise to it look like, and what the kind of dystopic solution dynamics could look like. So that we understand the underlying dynamics that have to be solved as a necessary but not sufficient starting point.

## Existential Hope

I want to share a few things in preface before diving in first. The concepts that i'm sharing are not my concepts I claim no propriety i've been fortunate to be in dialogue for many years a lot of really important thinkers in the field of existential risk in the environment and economics, and I also study a lot of people's work.

So i'm hoping to share some of the frameworks that I find most useful that is mostly a synthesis. I wish I could do good attribution in real time for all of it but in a podcast form where it's going to be edited it's tricky. So what i'd say is I have a blog there's a [resources section](https://civilizationemerging.com/resources/) you can look at the books articles, collaborators there and you'll see where a lot of the provenance of the ideas came from.

You can also go to the [consilience project](https://consilienceproject.org/) and the articles there are all well cited and you can also see some of the team and advisors people i've worked with. So that's just one important part.

The next important part in preface is I absolutely don't claim that the frameworks i'm sharing are complete or adequate to make sense of everything. I'm continuously refining my own understanding of this process. So this is halfway through 2022 that's my best current understanding of things i've no doubt that that will improve. That's actually one of the reasons to share this. So that if people have thoughts on better frameworks or errors that are here or additions that more collective intelligence can be working in this direction of what are the most fundamental things to think about and work on for advancing what is most critical that's not already being tended to.

I also want to share that when we think about global catastrophic risks and global existential risks in particular it can be a very overwhelming and disheartening topic, there's just a reality. My hope is that I would neither leave anyone in a disheartened place nor do I want to kind of present the bypass that there isn't a reality to feeling you know whatever feelings you feel about the real challenges in the state of the world.

For what it's worth i've been focused on these issues for a very long time I felt existential angst and sadness and despair and fear and all kinds of things about it. I continue to work on it because I also feel hope and a real sense of possibility to navigate through these things. So for whatever it's worth if you're newer to this study, as someone who has been looking at accidental risk for a very long time across lots of sectors – I will just share for what it's worth I don't feel that this is a hopeless situation. 

I wouldn't be sharing here if I did I feel like the solutions that are needed very much are threading the eye of a needle. But I both have a sense of possible solutions possible architectures of a world that can be stable metastable in the presence of exponential technologies and pathways to enact those solutions. It's not a given that we will, but it's also not a given that we won't. It feels still very much like a thing that our collective choices are determining the outcome of.

So the hope in sharing this is that more people's intelligence and care makes a difference in navigating these things. I also want to say that there are a lot of really important things that need done other than trying to figure out meta-systemic stuff about how to navigate the metacrisis. So if there's work that you're doing that is aligned with this whether it's work in animal rights or environmental work or social justice work or you're making intentional communities to prototype some things or community gardens. 

If that feels like what is yours to do and a meaningful application of your agency, I don't want focusing on the scale and complexity of these issues to be disheartening or make it seem like what you're doing is less meaningful. All those things have to keep happening and are really important.

For some people who are oriented to try to work on how do we change our macroeconomic systems our governance systems, our collective sense making and choice making systems the way we think about problem solving writ large the way we think about tech or at large. What we think about education the way we deal with regulation of exponential tech - hopefully this is useful there.

Even for people who are not necessarily going to be working in that area but just having a better sense of the things you're already, sensing and intuiting in some way is at least helpful for orientation that would be my hope.

## Interconnectedness

I think for the people who are probably drawn to read this I might share frameworks and details that you're not familiar with but it probably resonates with an intuition that many people have. I can share a little bit about my journey of like what got me into thinking about this because I think it's actually the things that got me thinking about are pretty common. I think I had some uncommon exposure to things that allowed me to find different frameworks and put more time into the topics. I was exposed to environmental work and animal rights work and extreme poverty work and things when I was pretty young.

My parents were into the topics and just getting engaged in kind of the field of activism. You see a particular thing like the dolphins in the cove or factory farms or what's happening in the amazon or the congo or whatever and that one thing seems worthy of the dedication of a whole life it seems so um catastrophic and unacceptable. Then when you start to see lots of those things, there's some sense that I had that I feel that i've heard many people have of like they seem somehow interconnected they seem somehow like expressions of some underlying patterns of how humanity is operating that collectively are reaching a point.

We have eight billion people, globalization, high technology, high resource use per capita, that they can't continue this way. They're not just a bummer anymore they're somehow kind of self-terminating and all somehow interconnected. I think that in some sense that there is a fundamental kind of reorientation that is not just a continuation of the dialectic of progress but is a kind of epochal phase shift of some kind.

It's about this type of epochal phase shift I think an intuition like that is not uncommon, that was there from pretty early in my exploration of the topics. Something that was maybe a little bit unique in my experience was I was engaged in these areas of activism but also because of being homeschooled. Because of what my parents were into I was reading [Buckminster Fuller](https://en.wikipedia.org/wiki/Buckminster_Fuller) and kind of design science considerations reading [Fritjof Capra](https://en.wikipedia.org/wiki/Fritjof_Capra) and the kind of ecological systems thinkers that talked about the inner connectivity of it.

Reading [David Baume](https://davidbaume.com/about/) and Krishnamurti[](https://jkrishnamurti.org/) who were talking about the fragmentation of our worldview being underlying to all of the problems. The ability to identify with an in-group and go into rivalry with an out-group, the ability to optimize a particular metric while externalizing harm to other metrics. That all the ability to benefit the near term while harming the long term that all of those were special case they were individual instances of the general thing. Perceiving parts rather than wholes where we would try to optimize parts in ways that damage the wholes which ended up actually damaging the parts.

Those kinds of world views I was exposed to early, and so I was thinking about what are the underlying patterns of all these problems what could solutions to those things look like.

## Failures of Activism 

I got to see that some of the really well intended activism that did actually succeed in solving particular problems ended up unintentionally causing other problems in the process. I think there'd been a lot of critiques of philanthropy and people have come to understand this that i've spoke about this in other places. But the first one where this really hit me and then I saw this everywhere was a project that was working on stopping elephant poaching in a particular reserve.

When you see how sentient elephants are, when you see how kind of gruesome the poaching is for making rings or whatever out of ivory you want to just stop it is a very reasonable response. But the solution was bigger fences around the preserve to keep poachers out and legislation for harsher sentencing for poachers for elephants particularly. 

After a huge amount of work to have those things occur and then to see the poaching of the white rhino and the mountain gorilla increased from some of the same people because the underlying poverty of the people hadn't changed a macroeconomic system that creates poverty at scale hadn't changed. The value systems towards animals hadn't changed an economic system where animals alive are not worth anything on anyone's balance sheet, but animals dead are. 

All those underlying drivers hadn't shifted and the other animals that were being poached now were actually more endangered and so not only did it not shift the underlying causes it arguably made the the whole global situation worse - and that was really devastating to me. Because like the first devastation was seeing these issues at all. It was factory farms for me was the first one, and so then the solution was there is a thing called activism and we can make the things better. 

The next devastation was seeing that the way we would try to make it better often made it worse, and not in an obvious way because it did make it better for those elephants there -  but as we zoom out if our goal is not these elephants here but is the kind of thriving of all life and perpetuity as best as possible then the way we define the problem has to get deeper.

So we see that GDP as a metric of the success of a nation is not an adequate metric, it does seem to in so far as you believe market ideology - that GDP going up means more products and services that improve people's quality of life goes up. It seems like a good idea insofar as you realize that war makes GDP go up, and sick people engaging in more health expenses does and addiction makes it go up you realize okay the GDP corresponds to environmental and sustainability GDP is not an adequate metric in the same way.

That's the case we can start to over focus on the metric of these elephants here or carbon reductionism the whole focus is get co2 out of the AIr but we can do that through mechanisms that cause other environmental or social problems. There's a need to kind of step back and say our approach to problem solving is actually one of the problems.

*One of the underlying kind of generators is because we define the problems in too narrow a way and so when we define a problem in a narrow way then we can create a solution to that narrow problem that does benefit that thing but that interacts with a lot of other things and ends up causing externalities or harm in those other areas*. 

So when we think about defining problems in a narrow way in a world that is actually interconnected, and we're separating out from that interconnectedness a particular part that we care about want to advance we can.

This is kind of what is unique to humans - our capacity to understand particular parts with abstraction in a way that allows us to build tools our our technology creation capacities are one of the obvious things that is unique to sapiens - also allows us to advance parts irrespective of and in ways that are harmful to wholes that end up creating these other issues

So one of the defining kind of features of the metacrisis that became clear early on for me was that our way of thinking about problems is one of the problems and that we actually have to have a much more holistic way of thinking about. What is worth advancing what are all of the things that are driving a particular problem, and what are the possible things that will come out of a particular solution that might create other problems to other metrics to other areas to other agents you know to other beings and how to factor all of that into what solutions are actually solutions worth advancing.

It was long tangent on saying I think that the intuition that we're at some kind of inflection point, that there are is something about the nature of how we think about things and how we go about solving problems and how we go about advancing things. That there is some change that has to occur at a deep level. I think this is not an uncommon intuition people will kind of come to things like motivating patterns of human behavior, seeming like the obvious places to start the kind of real politic assessment. 

That humans are too irrational to make the good choices and or too rivalrous to make the good choices and that we're in problems kind of as a result of us being very good at making very powerful tech, and not very good at long range and more inclusive very powerful tech and not very good at long range and more inclusive descions.

What I hope to do is to deepen some of these and clarify them in ways that might actually start to highlight what adequate solutions that are actually actible could look like in terms of 
a high-tech civilization, that are actually responsible stewards of the power of that tech.

## Catastrophic Risk

So as we dive into assessing where civilization currently is, I want to talk about the actual catastrophic risks that we face give some examples of them, and some timelines what is actually novel about this historically and what the support tends for the future. If we want a different feature than the path we're currently on what it might take to do that. I want to give a preface first that when we try to do any kind of like long arc of history big picture orienting frameworks. 

We have to be careful with it, it's easy to make those over simplifying and then become a source of confirmation bias historical narrative that often occur here that I think are both problematic.

One is a Hobsian dialectic of progress perspective - that pre-agricultural early people had lives that were brutish mean short and nasty and it's been kind of a upward ascent from the mud since then. That technology science and capitalism have been the candle in the darkness illuminating and making everything better, and that we simply need to keep doing more of those and it will continue to get better and better.

There's certainly plenty of popular books and popular thinkers that explicate this narrative and there's certainly truth to it. Nobody wants to go to dentistry and a time before novocaine. That perspective generally leaves out the unbelievable amount of species extinction and environmental harm and increased movement towards catastrophic risk, leaves out ubiquitous mental health issues that were probably not evolutionarily prevalent in the same way and lot lots of other things like that. It has a cartoonishly negative view of past, people. 

Also a homogenizingly cartoonishly negative view the other one is the view that kind of over romanticizes the living in perfect harmony and balance with nature indigenous people and that it's been kind of a fall from the garden or fall from redemption type model and that we have to get back to that early wisdom. I don't think that the way that people lived hundreds of thousands of years ago with relatively much lower tech much lower populations much smaller population sizes, or that they had adequate solutions for what we do with a population of this size and global supply chains and exponential technologies.

That doesn't mean it has nothing to share, I think there is actually a lot that can be shared but we're not going to find adequate solutions there I think. First it's just valuable to see that if you look around the planet today and take samplings of people from very different parts of the world or different subcultures they're really different right? 

Similarly if you go back into the past before the plow the idea that they were all kind of brutish nasty and mean or all kind of enlightened is pretty silly there's probably a pretty wide distribution of the values and the sustainability and the enlightenment if you would call it of the people. We see that early civilizations rose to certain kinds of heights and then collapsed.

So we also see that there was neither a kind of continuous descent nor continuous ascent or something that had cycles in it where how much knowledge was lost in the burning of alexandria how much was lost in the fall of sumeria or other places. When we see things like the anti-catheria mechanism which is like a metal geared clock that is thousands of years old that was not supposed to be the case or like the historical narrative of just kind of progress not probably not very true.

So it's important whenever we try to do these kind of sweeping assessments that we understand that at any given time there was a pretty wide distribution of the different types of civilizations and through time there would be fluctuations within those.

That said, I think some patterns that we can see pretty clearly is that from all of recorded history that we have any good evidence for the total level of technology that we have possessed since the industrial revolution and progressively throughout that time in particular since the digital revolution is historically unprecedented. The total population size and the interconnectivity of that globally through global supply chains is also totally unprecedented.

I think that's pretty straightforward, that's not that hard to argue. So I want to talk about the state of catastrophic risk facing the world today and kind of a historical perspective on that. It's fair to say that human induced existential risk because we always face the possibility of a meteor or a solar flare or something outside, like some kind of natural phenomena that could wipe us out.

But that's different than us doing something that wipes ourselves out. So human-induced catastrophic risk can be said to have started in world war 2 with the nuclear bomb. That before that we just didn't have enough technological power to destroy that much that quickly even if we wanted to. That as effective as he was Genghis Khan still couldn't make the planet not habitable and so one important insight when we think about catastrophic risk is human-induced catastrophic risk is it is inexorably tied to the level of technological development.

Because the technology is the amongst other things is a lever for how much power our choices have it also ends up influencing the type of choices we make which is an important point we'll get to later. But to just start with for now it obviously impacts the scope of our choices. People with stone tools just can't destroy the biosphere and just can't create a world war that destroys everybody. People with bronze tools or iron tools also can't, people with nukes can and so the nuclear bomb was the first time.

People recognized this at the time, its like when you see a mushroom cloud and you then hear the stories of Zeus's lightning bolt or whatever the mushroom cloud was a bigger destructive force than most of the kind of mythos of what the power of gods in different cultures had been. So it's like oh we have something like the power of god so we can split atoms and destroy things at an unbelievable scale and then we can go into something like mutual assure destruction and do that at a global scale - do we have something like the wisdom of gods and the love and the prudence to be able to steward and guide the power of it adequately?

That's one of the kind of deep orienting questions and frameworks that I come back to. So it's fair to say that the bomb was the beginning of the level of technology where human-induced catastrophic risk was actually a thing. It's important to say that before human-induced catastrophic risk was actually a thing, before that it was it pretty much happened to every civilization.

If you read books like [the collapse of complex societies by Tainter](https://www.goodreads.com/en/book/show/477.Collapse_of_Complex_Societies) that are doing analyses of the fall of previous empires, one of the things - and you just kind of think of the history of it's one of the things that most empires have in common is that they don't still exist that they either went through gradual decline or quick failures - which can happen sometimes they happen militarily.

Sometimes they happen through their own environmental and sustainability - topsoil depletion using up all the trees things like that. So I think it's an important point that if people are not thinking about this generally they might not be in the front of their awareness. Human civilizations have had lifespans and they've ended and they have largely ended from self-induced causes that either they were they would grow in a way that was unsustainable to the environment and collapse for those reasons or they would engage in resource conflicts to be able to have continuous growth that engendered more enmity with others that eventually led to their war.

Many of them fell from rivals that were less capable than previous rivals that they had defended against because they went through a internal erosion they became so powerful that then they started infighting. Having things like polarization and lack of coordination where then even smaller rivals could fight them so there was something like institutional or civilizational decay that occurred as a result of their success.

So it's important to get that the early civilizations went extinct and they went extinct largely for self-induced causes, that a enduring human civilization is a thing that we don't have a model for so far. What is novel now is that we have a truly global civilization for the first time. That as big as the aztec or inca or egyptian empires or even all the way up to something like the roman empire it was not truly a fully global system, given that the computer or tv that you're probably watching this on required six continent supply chains to be able to build.

Meaning that the the fundamental technologies that mediate the way that we live today require globalized supply chains that no countries can make the technology by themselves that they need, we really do it is fair to say - we don't have a chinese civilization and a u.s civilization since they are not autonomous. 

We really do have a global civilization that I would say is also in the process of self-induced collapse, and that the scope of that collapse rather than just kind of local environmental destruction something like destruction of the habitability of the biosphere at large or rather than a local war something like you know totalizing war is really unprecedented.

But of course those those civilizations did face the experience of existential threat to themselves, so it is important to understand we're not talking about like some unprecedented kind of excessively negative thing. We're talking about something that has happened a bunch of times, just never in a fully global context.

So then we get to world war 2 and we say okay so the bomb gave us the ability to have global scale catastrophic risk for the first time. That it became very clear we had to actually for the first time in human history we had a technology so powerfully destructive that we had to ensure we never used it and before. That every time we had a technology there was a race to deploy it as quickly as possible for everybody to be able to use it for advantage.

This is the first time we had a technology that rather than race to use it we had to ensure that nobody ever used it. Now of course there was still a race to maximize the total amount of stockpile and things like that and so an entire world system was created after world war 2 to prevent the use of the bomb again, and I would say that that post-world war 2 system was successful at preventing nuclear exchange in world war 3 of that kind kinetic world war 3 up till now. I would say that it also advanced it was responsible as a solution to a problem that was too narrowly defined that caused other problems. That world system was also responsible for advancing many of the catastrophic risks we now face and that that whole world system is ending.

## The post World War 2 system

So the post ww2 system of managing nuclear weapons appears to have been succesful, so far it seems, this was a world system based on mutually assured destruction. So how do we make sure two superpowers are not going to use their nuclear weapons on each other? Let's make sure everyone believes nobody could win such a war, so everyone would loose.

Another part of the post ww2 system was a new global monetary system called [Bretton Woods](https://en.wikipedia.org/wiki/Bretton_Woods_system) which was a kind of reserve curency system that allowed for exponential growth of the global economy. So one of the main drivers of war previously was war over limited resources i.e. battling over how much of the pie everyone gets. But in this new model, the idea was to grow the whole pie itself, so that everyone can gain more resources without needing to go to war with other states as happened previously.

However where do all these extra resources come from? So as mentioned previously, in solving one problem we have just 'externalised' it to some other area, so in the search for continual economic growth this has mean the depletion of limited resources and the pollution of the environment etc which leads us to hit the planetary boundries we are soon to hit. So exponential growth of the monetary system year-on-year means exponential depletion of these resources, exponential pollution etc, which of course is not sustainable and cannot go on forever on a finite planet.

So we have an exponential financial system built on a linear materials economy, that is taking unrenewable resources and turning it into unprocessable waste, which brings us to multiple planetary tipping points such as the depletion of topsoil, trace minerals, the accumiliation of co2 and mining waste, microplastics in the ocean and more. But we can see how these environmental problems are side effects & unintended consequences of the solution for how we avoid a nuclear war or other wars.

Another major aspect of the post ww2 system was globalisation and radically interconnected global supply chains. So the rationale would be, since we are so dependant on each other we have less incentive to bomb each other because we would be bombing our own supply chains. 

So many in the environmental movement want us to move back to more local production, yet what is important to recognise about globalisation is that it means we are invested in each others success rather than bombing each other, but it also has produced a world where collapses of supply chains anywhere can lead to cascading catastrophic shocks everywhere which we saw really clearly with Covid for example. 

Stopping the movement of the virus also meant shutting down supply chains, which meant ferilisers and pesticides didnt get where they needed to which meant for example devestated crops in many regions, crop failureswhich has led to food shortages for hundreds of millions of people. So in this setup local issues really can lead to global issues.

So these are some of the main aspects of the post ww2 solution that while they served a purpose, they brought us to the point that now we have reached planetary boundries across different axes where the unrenewability of resources and the unsustainability of pollution issues means we cannot continue to grow GDP exponentially on a linear materials economy. So what do we do instead that then does'nt also cause war from shrinking GDP or a lack of resources? 

This is where we need to try to hold the various tensions together and not just say ok lets prioritise the environmental issue by say lowering GDP, which is a difficult thing to do anyway to get everyone to agree, and whoever does it voluntarily dis-advantages themseves so few are going to do this. But even if someone does, this only adds pressure for how they will continue practically to supply the goods and services people need to survive without leading to shortages and the pressures for either internal or external conflict.

Therefore we need to think about the environmental, economic and geopolitical issues alltogether to avoid falling into these 'traps'.

The next part of the post ww2 system that is no longer viable is the idea of mutually assured destruction as a way of forcing a kind of equlibrium or nash equilibrium where you have one catastrophic weapon and two players and its easy to monitor from space say because there are not many uranium mines, however when you only measure catastrophes in this way then you start to get a world with AI or bio weapons, and many more types of weapons and players not all of whom are nation states, so you cannot game or estimate let alone establish a nash equilibrium so easily in the same simplistic way.

## Exponential Destructive Technology

So we have advanced to a world where more states have nuclear weapons, and other states have other means such as disinformation attacks which could lead to nuclear escalation between countries that do have them. While nuclear weapons are hard to build, other types of weapons are much easier to build and aquire such as AI swarm intelligence powered drone weapons which can take out massive infrastructure targets - but can be built by small groups of people, similarly with bio tech weapons for example with technologies like crispr and others are advancing faster than moore's law.

While we hear postive things from silicon valley about the democratisation of power being a good thing, perhaps the democratisation of catastrophic power is not such as good thing that could be used intentionally or accidently? but it is also much harder to monitor than say nuclear weapons, and where you might have an even harder time trying to enforce international agreements to limit this use.

This creates a radical fragility in the world so even small actors - not nation-state actors with no special exotic materials can increasingly easily aquire these technologies in AI, cyber and bio tech. 

How does the world make it through this decentralised catastrophic ability for anyone that wants it? Especially with technologies such as AI you may not even need to build it yourself, or require any hardware or software yourself with access to cloud computing infrastructure that is available to everyone you can simply rent that capacity relatively cheaply.

Even scientific publishing portends to catastrophic risk, because say if a major institution is advancing knowledge of how to do swarming drones, even if this work is reviewed by ethical review boards as work being intended for an ethical purpose, this can nevertheless cause unintentional harms in other ways as its very easy to re-purpose technology for uses it was never envisaged for.

So we are in a situation that is so radically unprecendented not just before ww2 but in the world after ww2 until just recently, where even open access to knowledge and information in fields of advancing technology has growing risk potential that is effectively unmonitorable.

So we can see we cannot use the previous model of mutually assured destruction in the current landscape - nor can we keep an exponential growth economy with linear supply chains without catastrophic risks. Many of these are tensions with each other where for example decreasing the likelihood of war might increase environmental risk for example, or vice versa.

## Near term risks

The post ww2 pax americana world is effectively over and is inadequate to the landscape of the problems we face, the problems we face are historically novel and the near-term catastrophic risks are not on one axis (e.g. just nuclear) but on many different axes without adaquate solutions currently and so thats the basic foundations of what you might call the Meta-crisis.

So what could be some possible near-term catastrophes? In order that we can understand what needs to be addressed. [Nick Bostrom's work](https://www.goodreads.com/book/show/2659696-global-catastrophic-risks) and places like [the future of humanity institute](https://www.fhi.ox.ac.uk/), and its worth checking out his [vunerable worlds hypothesis papers](https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12718) would be a good start.

Lets take climate change. There is a pretty radical debate on how much temperature change, sea rise etc comes from this, but there are increasingly severe weather events seems to be clear over the last few years, and everything indicates they will continue to increase and create catastrophic risk. The next few summers could have heatwaves that displace large numbers of people much larger than ever before, tens of millions of people is not unlikely for example.

We can also observe from recent refugee crises like Syria that no country seems interested in taking tens of millions of refugees. Then if we consider somewhere like northen India, Bangladesh and Pakistan which has a huge population density a place where there are heat waves, potential food shortages, where tensions have been historically high already from previous conflicts. Then we can see the potential how further resource shortages and refugee crises in this tense area, with two nuclear weapon equipped countries, could be jusy once example where a climate crsis could lead to a nuclear crisis and escalation along existing lines of geopolitical tension.

There are a few situations like this that in the next single digit number of years have a much higher probability of escalating that are under mitigated.

Another example is since Covid a radical kind of polarisation in the US between left and right has expressed itself through the kind of Janurary 6th activity at the capitol and the huge polarisation around Trump, and George Floyd triggered social justice issues in the US.

As well as this, a descent perecntage of Austrailia has been catching fire and is barely even staying in the news because of other things such as the war in Ukraine and tensions over Taiwan, so he thinks there is a sense of instability but the complexity of it and that fact that there is not one catastrophic risk but many things happening simultaneasly interacting in complex ways, which is of course a huge challenge, as is the number of escalation pathways which is increasing each year rather than decreasing.

All of these are a result of tech and this is either the results of cumulative effects of industrial tech multiplied by globalisation having cumulative effects over hundreds of years and now reaching tipping points.

## The Energy Trap

So technology that recursively has the capacity for supporting its own self-improvement creates an exponential curve technologies that allow exponentially larger impact per dollar or number of people or unit time. So we see facebook and google getting to three billion users in like a decade compared to the scaling at which u.s standard oil got to nowhere near that many people over a much longer period of time we can see that a much smaller number of people can have a much larger impact much faster so exponential speed of effect exponential scale of effect which also means more players able to have a bad level of effect.

When we look at the myriad of environmental issues that we face that we said are a result of the cumulative effects of industrial tech with increasing population and increasing GDP demands it's illustrative to be able to have that kind of overarching framework to look at it as opposed to just individual areas.

Because if we look at say climate change as one environmental risk that is largely a result of excessive co2 from the burning of carbon-based things mostly fossil fuels obviously - there are methane and agriculture and other things but let's just take take the carbon part so as far as the supply chain goes that's kind of the waste or the output side of using hydrocarbons as fuel.

The other side is that we're getting diminishing returns on hydrocarbons that we have a society that depends upon hydrocarbons as the energy source and while we're figuring out creative ways like tar sands and shale and fracking and offshore drilling to find the rest of them. We're having to go to harder more difficult areas because we've already got most of the easy ones, and it takes a certain amount of oil to get more oil so the energy return on energy investment how much oil it takes to get a new barrel of oil is increasing.

When you look at the fact that the total amount of GDP is very closely coupled with the total amount of energy use globally and the GDP has to go up exponentially just to keep up with interest that means that there is a exponential demand for the total amount of energy used but we're getting a diminishing return on hydrocarbons. And the new non-hydrocarbon sources of energy have required more hydrocarbons to make and they have a certain energy return on energy investment of how long it takes you know for them to pay off - there's massive kind of gap and and reckoning on that side obviously.

As well as the underlying deep issues between the inner global energy system and the global finance system. There is also just the other side of the supply chain of climate change. 

So you can see climate change when you look at the entirety of the energy system you're like wow there's some other really really critical and timely issues that are just part of looking at the movement of hydrocarbons to the system let alone everything else. So if you want to think about all of the environmental issues from the point of view of nature having mostly closed-loop processes that anything any type of waste in nature is just food for something else in a time scale that doesn't lead to either waste accumulation or depletion of a needed resource.

So the idea of a linear materials economy that is undergoing exponential growth on a finite planet - that thing has to shift. So we both have to deal with the embedded growth obligation and finance for exponential growth, and we have to deal with the closed loop on our materials economy so that we are making all of our new stuff from old stuff and all of our old stuff is not waste or pollution but it's getting you know cycled back in to make new stuff.

## Peverse Incentives

When we think about the exponential tech mediated issues one framework for thinking about that is that there is a perverse incentive a kind of incentive. If people are not already familiar with the concept – it's another concept that you can see pretty quickly underlying most of the problems in the world not just the catastrophic risks just the suck. It's also that we have to deal with lots of industries perverse incentive means where any individual or corporation or nation state or agent actor of any kind has some type of incentive that is misaligned with the well-being of other actors, so they are incented to do something that is directly causing harm or externalizing harm somewhere else.

It's pretty easy to see that a for-profit military-industrial complex that would go out of business if there was no war or there were adequate solutions to peace or for-profit health care structure the way it is or whatever, we can see like perverse incentives are not a hard thing to see. 

So that would be an example of something that is kind of underneath and driving at the level of the nature of economic structure and how we regulate that and to really find adequate solutions. I think it actually requires changes at the level of how we create currency issues regarding the fungibility of currency and property ownership private property - deep deep topics. 

But if we were to think about what are some underlying drivers of the things that have been problems forever but at this level of technological capacity and nearing planetary tipping points. Where those problems actually become catastrophes - perverse incentive would be one of those drivers which then also means that it creates a basis for an orienting question, which is the orienting question would be for humanity to think about - how do we identify and close perverse incentive gaps procedurally everywhere?

So there's a particular kind of perverse incentive associated with the development of new technologies in particular which is that there is much more incentive to focus on opportunities than of a new technology than risks of a new technology.

So a new technology will obviously have both right - the technology it's only going to proliferate if it does provide some advantage so of course there's going to be opportunity that's associated. But because it's going to interact with a complex world there's going to be risks that are associated with it. If I focus - if I don't want to advance a technology that might harm the world meaningfully so I am going to do really deep risk assessment i'm going to think through what are all of the ways that this could cause risk both through its direct physical externalities as well as the ways it changes social dynamics. 

Because whoever uses it gets more power in some ways - so what types of people will use it how will it change the power landscape and how it will change the nature of psyches and things like that if I really try to do good thoughtful assessment of where the second third fourth order risks associated are? 

And either not build the technology if they are too bad, and I can't mitigate them or figure out how to design the technology differently to mitigate those or make sure that the technology also launches with particular regulation in place or whatever. Then i'm probably not going to be the first to market I might not get to market at all I might just decide not to do the thing.

If on the other hand I just focus on the positive application, and probably whatever the nearest term most profitable positive application is and I either don't do risk assessment at all or I do a pro forma version that is mostly about box checking to say that I did risk assessment while really not wanting anything that slows the movement. Then i'll probably get first mover advantage. 

I'll probably get the ability to scale and especially in network dynamics be the one who gets to benefit from [metcalfe's law](https://en.wikipedia.org/wiki/Metcalfe%27s_law) and kind of network monopolies.

As a result there is a kind of race to the bottom of anyone who would want, who's aware that there are risks either doesn't matter because they won't end up getting the power or they end up having to say well I can't focus on those now. Otherwise I won't be a player and I won't be able to do anything so I have to I have to pretend that the risks are going to be solvable later and still rush to get ahead.

So at least then I have the the capital in the market position to hopefully be able to implement solutions later, which of course is pretty much impossible because then there will be a fiduciary responsibility to continue to maximize shareholder advantage there will be inertia already in place for example.

## Private gains, external losses

The incentive to move fast and break things externalize and socialize the losses but privatize the gains and that even if I feel like I want to be an ethical actor I still want to be able to do stuff and so I want to gain as much power as I can. If I don't do the thing it still doesn't prevent the thing because other people are going to do it anyways.

How we make it through gaining exponential influence capacities across domains as fundamental as the genome and the base code of life and artificial intelligence getting increasingly generalized, and things like that like this is a very deep fundamental thing to deal with.

So there's a great video has somebody put out on [leaded gasoline and the development of the lead additive that made a motor stop knocking and the actual health risks that were known even to the scientists at the time](https://www.youtube.com/watch?v=3Cy02SmP0co). That they lied about and that led to leaded gasoline proliferating globally, that led to [huge amounts of lead toxicity in the biosphere not just affecting human life but also human life[(https://www.youtube.com/watch?v=FpS24un842k) and that a little bit of the effects known of that was that the lead decreases human IQ significantly and the collective effect of that across the whole population is something like billions of points of IQ loss globally. 

Lead also makes people more violent I think it was something like a 4x increase in violence being exposed to it again across the entire populations like global populations. We come back to think about how a lot of people like to write off that we can't make a better civilization because humans are too dumb and too nasty, and we see just lead makes us dumber and nastier, and we're like well how dumb and nasty are we is not a unchangeable metric it's actually changeable by a lot of things

There some cultures that have developed a much higher level of base education across their whole culture yes you know like jews have done a better job of education than a lot of other cultures have are there some that have done a better job at non-violence for example. 

So this portends a radically different approach to the relationship between the market technology and regulation and whatever culture ends up in forming regulation.

## Biotech Risks

So with biotech risk there's a lot of different things we're talking about we're talking about things associated with synthetic biology which is advancing faster than moore's law and things associated with crispr and different kind of gene editing technologies.

Both as a result of things that could be used for positive applications like agriculture or waste management or whatever but that could produce unintentional consequences as a result of putting you know genetically modified organisms designed to benefit one thing, but that start to proliferate and that you can't kind of pull back afterwards it might have other effects we haven't studied well enough.

There's the risk that we're developing things that we know aren't safe for release but they get out because containing tiny microorganisms is a really hard thing to do and so just accidents are a very real thing. And then obviously bio weapons and not just state actors that you can kind of deal with via deterrence. But non-state actors where it's very hard to deal with them via deterrence because they might not care and or might because they're a suicide cult or whatever and or because they're not identifiable.

When we see increasing kind of suicide shooters where you know they can kill a lot more people than assault rifle than they could with a handgun or with a knife as we move from assault rifle to weaponized drones or biotechnology or whatever obviously how much harm a psychologically unwell person can cause as the decentralized weaponizable technology increases changes a lot.

So far there's been this fortunate thing that the people who are good at tech and who are good at strategy have mostly been gainfully employed and not in the place of want to burn everything down. As we're moving to a place whether it's through things like radical polarization where both sides feel that elections are existential, and they feel that there is some kind of grand force harming the world irreparably. 

Or whether it's through way more people being disenfranchised through things like climate refugees and through the effects of kind of the social media orientation, as it is optimizing for engagement that is largely achieved through getting people more limbicly hijacked and tribal and doubling down on confirmation bias.

The total number of people that feel existentially disenfranchised and are motivated to take some kind of action like that that intersects with the number of people that would have catastrophic capability. Those two circles are moving closer together so there's more in the venn diagram and the more people with catastrophic capability is also growing exponentially so that's a increase in the vulnerability of the world. 

I think in his conversation with [Sam Harris, Rob Reid talks about one of the examples of concern being a project that is happening at the time of filming this to try to that actually a u.s government department is engaged in trying to get all of the bio labs that do gain a function research to publish all of the sequences to some kind of open source database](https://www.youtube.com/watch?v=UaRfbJE1qZ4). So that the knowledge is more available for preemptive vaccine creation so that if some future pandemic occurs we'll already have vaccines in place. 

Totally understandable to want to do something like that to want to take the collective knowledge that humanity knows and centralize it so that um so that we can coordinate on solutions like it seems like a good idea. Though of course that also means making more centrally accessible and maybe even open source access to all of the catastrophic bioweapons capability for anyone who would use it for bioweapons purposes. 

In a world where the technology needed to build that is getting increasingly easy that's just an unbelievable risk. So this is again an example of the way we try to solve a problem can make way worse problems. 

So one of the things you want to think about is what are the possible negative externalities of our solution whatever our solution is one. What are the what are the upstream causes of the problem we're trying to solve and have we address those upstream causes, if not what do those upstream causes do when this isn't their outlet.

## Artifical Intelligence

With regard to AI we can see that the AI curation algorithms that run social media and search engines and infotech input technologies generally are already having a civilization altering culture effects that is particularly heightened in democratic societies. 

This is the work of Tristan Harris and people like this got plenty of stuff on the internet for people to check out if you haven't seen social dilemma. Briefl, that the ability to take kind of all of the information to say say we're talking about facebook you've got billions of people incentivized to upload stuff incentivize by getting likes and you know whatever and that's to create their own content or curate existing content.

So you've got billions of people creating content, and then billions of people engaging with that content and where what is going to show up in my news feed is what is optimized to make me spend the most time on site and engage with.

The feed the most empirically addictive content, as it exposes me to different stuff in the news feed and it empirically measures what I would engage with the most and it happens to be that consciously I probably did not plan to spend an hour on facebook today as I was planning my day.

I probably wanted to check it briefly and go to other stuff. So if I stay kind of an executive function i'll probably get off facebook. IfI get limbicly hijacked through you know a bunch of hyper normal stimuli aIrbrushed pictures and whatever and things that outrage me i'll probably spend more time on site.

If I get confronted with views that are complex and make me think i'll probably bounce, if I get things that kind of appeal to what I already think and enrage me i'll probably do it the clickbaity titles get clicked on more. 

So without trying to there is this kind of comprehensive appeal to the lowest angels of our nature that occurs through an AI. It is a advanced artificial intelligence that is doing curation algorithms. It is making almost everyone more certain, more sanctimonious more righteous more tribal,  less open-minded more villainizing across all the views. It's not left aligned or right aligned or whatever - it's engagement aligned for everybody. 

That's already an example of catastrophic risk happening globally then an increasingly polarized population elects an increasingly polarized representative class. The increasingly polarized representative class in a democratic system means more gridlock and inability to do anything. Which means inability to regulate exponential tech, or solve climate crisis or environmental crisis or engage in the great game of geopolitics with china that doesn't have the same issue.

So we can see like real true catastrophic risks at the level of the environment and geopolitics as a result of the AI that is already employed in what seems like benign entertainment. Channels or whatever, and that's extremely low and that wasn't intentional right - like it was not designed to destroy democracy it was designed to do the very basic functions of design do those are second order effects.

As we look at things like gpt3 that is already able to generate novel text that passes the turing test that people read and think it was a human that wrote. Because it's good enough and can already generate novel text of specific types like in the voice of a particular person on a particular topic by in in taking all the writing of that kind and again.

Though the rate of growth of generative AI of this type is faster than moore's law right now. Which both means how powerful it is and how widely available it's becoming. So a lot of people have started to think about the kind of deep fake apocalypse of when content can be generated.

Not just now curating existing content, but generating bespoke content that can be used personalized info about people or groups to put forth content that would be maximally appealing to them that can utilize statistics and images and whatever but that are all created.

How does anyone make sense of anything in a world where I can have more fake information than real information, can't tell the difference and the fake information is more compelling? 

Now you start to imagine you have the curation algorithms and the creation algorithms working together to be able to both show me the best of what has been created by humans and and then creating specific content, and creating those together. 

So you can see like a breakdown in public sense making tribalization polarization those types of issues associated with AI.

In general if you think about say alpha go google's deepmind's technology, they can beat the best human player at chess and the best human player at go and the best human player at starcraft in any definable game.

It's already not just a narrow intelligence right it's already a narrow intelligence across any sector that you want it to be that has radical supremacy over humans and the speed at which it develops the capacity to do that is is mind-boggling.

So as you think about could a game be made of turning lots of metrics that you'd want to optimize for into games and figuring out how to win those in terms of applications in militaries and markets and public opinion. Whatever that technology is - it's not an arms race between nation states so that's also happening it's a market kind of arms race between companies competing for getting there first.

But there is definitely an all-out race to advance the most powerful technologies of intelligence and obviously you have a artificial general intelligence case where it moves completely out of our control and maybe it's not possible for us to create alignment between its intent and ours and that's kind of the biggest risk. 

But even the one where it stays within our control but it is radically leveraging human choice this is one of the things when we say we've got the power of gods without the love and wisdom of it. Hstory doesn't show humanity being particularly good stewards of its technological power in any meaningful definition of good. That we use increasing technological power to increasingly exploit the environment, other classes other people it's pretty clear.

What we could say is that we don't get to continue to utilize the power of technology in those types of ways as exponential technology comes online without self-terminating. So the type of mind that gives us the ability to make the tech is not the same type of mind that gives us the ability to regulate it and steward it wisely.

That type of mind being able to catch up with and guide direct and bind the other is critical to humanity making it through its technological adolescence.

## Multi-Polar Traps

So this brings us to the topic of multi-polar traps this is a underlying feature of what drives many of the major problems in the world both in terms of market issues environmental issues military issues many things like that and without being able to solve this underlying feature more fundamentally and categorically none of the specific areas find adequate solutions. So I want to make sure we understand this if you want to read up more on it there's a exceptional paper online called [meditations on molok on slate star codex](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/) that's probably the best overview on the topic i'm aware of.

So what do we mean by a multi-polar trap? we mean a multiplayer prisoner's dilemma or a situation in which you've got a number of different actors that could be different nation states different corporations different tribes whatever it is who can be in a competitive dynamic with each other. 

Where if any of them do a particular type of action that if everyone does that will kind of create the worst case for everyone long term but will create so much advantage for them in the near term that they will win enough power that everybody else loses if they don't also do the thing.

So if anyone does the immediately advantageous though long-term harmful thing everybody else has to race to that thing.

So let me give some examples say there is a new type of technology that emerges that has the capacity to create a more powerful type of weapon and if anyone starts advancing the technology for weaponry in that way they will win the next war. As a result everybody has to not only advance that weaponry, but they have to try to race to advance it more powerfully than the other guy faster.

Also a whole suite of other weapons that are like counter weapons and defenses against that thing this of course increases the lethality of the entire world in the lethality of the next war for everybody but if we can't prove that the other guy isn't going to do it then we all have to race together as fast as we can and this obviously happened with nuclear weapons this is even when we were supposedly doing nuclear disarmament nobody wanted to give up their last nuke. 

Which is why nuclear disarmament is so hard nobody wants to give up their last nuke first because what if the other guys say they gave it up and they didn't really? They actually have a nuke and some secret deep underground military base and then it's game over for whoever authentically gave the thing up.

If we don't have the transparency to prove that the other guy really gave it up or hasn't done the thing, then how can we ever create those agreements when there's so much consequence on the asymmetry of capacity in that agreement?

So even when we were supposedly in nuclear disarmament there was still an arms race happening to make faster and faster warheads the hypersonics because if someone had nukes that were way faster they could win first strike. So we see these types of arms races we see this right now in all the categories of exponential technology, a new type of computation or computational capacity comes online it leads to new types of cyber weapons. 

There's both a race on the development of the cyber weapons and a race on the ability to create defenses and hardening against that. The same is true with biotechnology as we mentioned earlier that there are real existential risks associated with many types of biotechnology from genetic engineering to synthetic bio. 

Many of these are not from weaponized purposes they're from positively intended purposes that have externalities or positively intended purposes like gain of function that have accidental releases but obviously weaponization is one of the possibilities. So again if they portend so much power if anyone might possibly be doing it then everyone has to race to do the thing and the counters to it.

Probably this is nowhere more pernicious than in the case of artificial intelligence and because artificial intelligence increases the ability to do every other kind of weapon and you can use artificial intelligence systems for developing biological and chemical and cyber and every other kind of purpose. 

There's this concept came from [John Boyd originally the concept of the ooda loop](https://fs.blog/ooda-loop/#:~:text=Developed%20by%20strategist%20and%20U.S.,%2C%20Decide%2C%20and%20Act.%E2%80%9D) - observe orient decide act the speed at which a particular actor in a military conflict can observe what's going on orient to it. 

Who's the bad guy who's a good guy, what's the right thing to do make a decision act and then do it again based on the consequence of the action the speed and accuracy with which they can do that determines who wins in highly unpredictable scenarios.

Obviously AI's are going to win ooda loops and so then you have ooda loops in kind of multi-polar traps - with each other leading to the development of maximally capable AIs and kind of multi-polar traps with each other. 

And you'd say well why not just make an international treaty that no one will build AI weapons that seems very straightforward the u.n could mediate it and we could all just agree nobody build AI weapons because really a general still a person with family maybe kids grandkids like who wants to live in a world with AI empowered autonomous weapons that have that speed of decision making and lethality?

But because we can't ensure that no one else is doing it because even if they say they aren't doing it how do we know for sure in some deep underground military base they aren't doing it. We have to assume that they are and then we have to assume that they are doing the most advanced version possible and then we have to still ensure that we beat them at it. Then of course they also might want to not do it but they have to assume that we are.

So this is a collective action problem, a coordination problem and we can see the application in arms races now as we mentioned earlier up until world war 2 in the nuclear bomb every time there was a more advanced weapons technology there was a race to implement it for the advantage it would give with nuclear bombs. You had a situation where the destructive capability could be omni-destructive that you had a race to develop more and more potential capacity to use it while trying to ensure that no one actually used it it's a weird situation.

We're still in the place of trying to develop the potential capacity to win just through deterrence threats you know whatever it is. But obviously our the total lethality and the total consequentiality of what could happen if initiated even because of accidents especially in a time where launching disinformation campaigns and cyber attacks on proper information. 

Or if someone else launched their thing or not it's getting increasingly easy for non-state actors it then affects everything else the precarity of that much destructive capacity in that many locations like it's just obvious right just to think about the precarity of even nuclear weapons.

If you haven't read the [doomsday machine by daniel ellsberg](https://www.goodreads.com/en/book/show/25663779-the-doomsday-machine). It's really worth reading to see how many times nuclear weapons almost fired just because of accidents like person accidents or computer glitches. It almost inspires some sense of like supernatural awe that we are still here. But increasingly so as we think about how many more catastrophic capabilities and how many more hands are emerging.

So the multi-polar trap is underneath why we can't say let's just not do a particular thing and we see that in this kind of military example here. You can take that all the way back to kind of early tribal warfare and let's say that there was a area where there were some tribes that were actually quite peace loving and not oriented towards warfare and maybe had some kind of animistic or spiritual ideals to want to live in harmony with nature or whatever. 

It was as much as they could if any tribe around them starts being oriented to tribal warfare realizing that the increasing population in the area is creating a competition for the dwindling amount of natural resources available fishing or hunting or whatever it is.

That killing people in another tribe means decreasing the competition for those resources plus getting the stuff and the surplus they've already created. Also that the weapons you use to hunt and the weapons used to go kill another tribe are not that different. If any tribe orients towards that every other tribe has to orient towards that or they lose by default. You don't get to just say hey I opt out of this game right?

So then that creates a situation where if some group orients towards that very powerfully right like Genghis Khan or the spartans or whoever who are going to invest most of their capacity in in military tech how does any other culture in the presence of them not get destroyed if it doesn't develop adequate military tech to at least defend itself ? 

How do you defend yourself against someone that is that exceptional in that without having that change who you are as a culture completely because you have to now create all the investment in that category.

So when we were talking earlier about kind of long arc of history the idea that warfare is one of the selection environments where cultures made it through or didn't make it through. The ones that were not successful at warfare in relationship with the ones who were waging it didn't make it through so there was a selection both for the capacity and orientation to be effective at it. 

Then the more successfully violent ones took out the less successfully violent ones, the successfully violent ones warning with each other upregulated each other's capacity in a particular kind of multipolar trap scenario. Then we are the descendants of those who made it through that.

It's an interesting and important insight you can see that for a couple hundred thousand years give or take of homo sapien history we mostly lived in these very small tribes you know below the [dunbar number](https://en.wikipedia.org/wiki/Dunbar%27s_number). 

Because people we can assume didn't want to live in much larger groups, where for the group to have coherence they would be bound by certain rules. That if the group was too large they didn't have a say and when you are below the dunbar number everybody can sit around a circle around a fire and all have a say on a big decision. So if i'm going to be bound to something at least want to say in it.

I want if i'm going to sacrifice for other people that they're people that I really know. As soon as the group gets too large i'm supposed to make sacrifices for people I don't know and i'm bound by rules I don't get a say in. The group would rather just cleave and make smaller groups. It seems clear that humanity stayed below the dunbar number for whatever set of reasons pretty rigorously for a very long time.

So that's very you know it's very interesting from like the scale at which humans evolved to be able to operate like our whole evolutionary genetic history was operating at that kind of scale mediated in in those ways. But of course amongst other things one of the things that would have selected for ending that would have been tribal warfare and if say a larger tribe wants to initiate warfare smaller tribes better merge together to be able to defend themselves. 

That means probably giving up some freedoms and quality of life. So as to have safety and security and now of course the race for larger groups with more effective weaponry and more division of labor to have more you know capacities and more surplus which means more extraction from the environment that race is on. 

So we can then kind of see the history of civilization in terms of the history of extraction capability from agriculture and mining and whatever. What we would call exploration capability which means being able to convert more of the natural world into stuff that we use.

The expansion of military capacities the expansion of coordination technologies the expansion of size of group that can be in coherence with each other to fight rivalrous dynamics externally. So eventually we got empires and kingdoms and the nation states and then kind of global economic trading blocks and nato's and things like that.

With the exponential increasing types of technologies that we have both for extraction you think about a mile-long drift net being able to pull hundreds of thousands of pounds of fish out of the ocean in a single go compared to a fishing line or compared to what an orca can do this is exponential extraction right?

And we can see that in every area of industry from factory farms to mining to drilling to fracking to whatever exponential extraction, exponential monetary creation, exponential information processing weaponry et cetera. But where there is some externality to the environment and to social cohesion and to sense-making happening everywhere you're advancing some things causing externalities but at you know exponential increasing scales. 

Historically what has won and made it through was an increase in the capacity to win at game theory and yet that is kind of a long exponential curve that is really verticalizing now. It's verticalizing in its total game theoretic which also means externality creating and destructive capacity while reaching planetary boundaries.

If we continue to do that which is always one which means win at arms races that process itself self-terminates and so this is a real tricky thing, because we can neither say well let's kind of do the luddite direction of like less advancing military attack or whatever and just lose that doesn't work. We don't want what happened between tibet and china has happened a lot of times throughout history and trying to do tibet doesn't work.

But trying to continue to win at game theory driving arms races that are increasing destructive capacity and externalities also doesn't work as you're approaching the point at which that creates inexorable catastrophic risk. So something that has never happened before has to happen soon.

There are some other examples of multipolar traps this is an example of kind of the arms race side. We can think about the tragedy of the commons as another multipolar trap and it's not that no multi-polar traps have ever been solved or bound they have. You can read [Eleanor Ostrom's work on the process of commons management](https://earthbound.report/2018/01/15/elinor-ostroms-8-rules-for-managing-the-commons/). 

You can obviously see how mutually assured destruction was binding a multipolar trap on nuclear weapons in a particular way. It's that we don't have adequate solutions to the nature of the multi-polar traps that are catastrophic that we face currently. So rule of law is a way to bind a multipolar trap right inside of a nation state, so that everyone isn't racing to cut down all the trees and there's no national parks we agreed to make national parks. 

Then create a monopoly of violence which is a police force to be able to back that up so if people try to go to do it they you know be physically forcibly stopped until we get to have some trees and that would also be true for crime and pollution, and other things like that where we use law and the ability for enforcement of law to be able to back it up within a nation state.

There are places where that's really hard to work because of course then the economic incentive is to try to capture the regulator the regulatory apparatus is trying to bind where there is excessive perverse incentive in the market right the idea of kind of a liberal democracy. This is usually, let the market do most things because it's kind of a decentralized collective intelligence where hopefully people demand real goods and services that will benefit their life which creates an evolutionary niche for people to create products and services as supply. 

To compete with each other to make the very best product or service at the best price, so that the rational actor will buy. That kind of like old-school market theory this is obviously not true - it has some truth in it. But it's not completely true as the behavioral economist showed people are not rational actors who make the the best choice especially in a world where nobody can even see all the choices there's a kind of information overwhelmed.

So the thing that actually is the most effectively marketed thing will end up succeeding over the most effective thing manufactured demand is very easy. So, rather than demand driving supply people now want that won't actually increase the quality of their life will empirically make their life worse, that they never wanted before because the nature of marketing manufactures demand that wasn't there.

The whole idea of the collective intelligence of a market is that demand drives supply like that's a foundational idea I would suggest as to why it is actually net good the moment that the supply side can drive demand for things that don't actually increase the quality of life meaningfully. 

Which is through largely appealing to addiction and keeping up with joneses and kind of lower angels of our nature stuff and of course as soon as corporations start to become large which earlier if you think of like a local market that wouldn't have been the case. But as soon as corporations become very large then supply and demand have a radical asymmetry and of course supply and demand should be the same size in aggregate. 

## Corporations vs Individuals

The supply side of a major corporation a billion-dollar corporation is coordinated as a corporation with a org chart and decision-making processes, and all of the consumers are not in some kind of like labor union for consumers some consumer union that give it has equal information processing to be able to play that game theory. 

It's the multi-billion dollar corporation against the individual in terms of the game theory so of course the corporation can employ behavioral psychologists and AI split testing and all kinds of things, to be able to optimize supply driving demand rather than the other way around. The underlying intelligence of the market is broken at that point and you really just have asymmetric power having captured the ability to maintain and advance its own power.

So the idea was in kind of formation of modern democracies the market should be largely free because it does a lot of good decentralized things we want to decentralize stuff as much as possible, democratize decentralized. But the market will eventually create a power law distribution some people will be better at market than others and as a result they will make more money and they will be able to use their more money to make more money.

So you'll end up getting rather than an equal distribution of money a pretty tight power law distribution of money and then the people with the money have maximum control and it'll become feudalism again. We're trying to get away from feudalism, so you have to make something more powerful than the most powerful wealth class if you don't want it to be feudalism again. 

Realizing that the wealth will follow a power law distribution, so the idea of let's make a state that has rule of law and has a monopoly of violence so that it is more powerful than the most powerful of the top of the market. Then the state is run democratically so that the collective values of the people and then of course this only works if the values of the people are somehow developed. 

For example all of the kind of founding documents in the US and other modern democracies talk about things like the number one aim of government should be the comprehensive education of every citizen in the science of government George Washington or things like that.

It requires a comprehensive education informedness, and some kind of moral education simultaneously for a democracy to work otherwise democracies are just a really dumb idea of lots of rivalrous uneducated people all having a say in how things go about that they don't know anything about Socrates is kind of critique of why democracy was a dumb idea in ancient Greece. 

So if you want to be able to democratize decision making you've got to educate and enlighten the people. I think it was Franklin or Jefferson's quote on if you believe who should the ultimate depository of the power live with, and if you believe the people to be too unenlightened to hold it, then the obligation is to enlighten the people because there is no other force that can hold it that doesn't become despotic these are paraphrases.

So the idea is a civilization you know our society that invests in the collective human development of the people that the values of the people then get encoded through some kind of democratic process into rule of law. The rule of law has a monopoly of violence so that it can then bind the predatory aspects of the market because mostly things that are agAInst the law are things where somebody has an incentive to do something but that we collectively agree we shouldn't let people do. 

But there's an incentive to do it which is fundamentally a market type force so let the market do a lot of things regulate the up things of course. The challenge becomes that the market where it would be regulated by the state in a way that is disadvantageous towards it has its own incentive to try to change how the state regulates and so this is where lobbying comes in this is where campAIgn finance comes in. 

This is where you know all of the ways in which regulatory capture by the market starts to come in and when you realize that the people in market positions have an enduring incentive to the market where the people in representative government positions don't have an enduring loyalty to that position.

This is the public choice critique of representive democracy, it becomes pretty easy for regulatory capture to occur, see heaps of examples of that. So there there was clearly some right ideas in those structures of what should be done through decentralized type process. Where does that still have failures, how do we figure out how to create some centralized power that still represents the decentralized will of the people to be able to create checks and balances on power coming out of the problems of monarchy.

Not having the noblest oblogations, that it was supposedly supposed to have the modern systems were very much developed around the idea of checks and balances on power so don't allow too much power to concentrate anywhere as that ends up having you know uncheckable corrupting orientation. 

So let's create a separation between the market and the state let's create a separation between the state and the church and other kind of civic organizations let's make sure there's no monopoly on religion so all of the religions are allowed. Let's make sure there's laws against monopoly on corporations so the competition between corporations can check each other let's split the government into three different branches um you know on and on.

This all designed to be how do we create checks and balances on power so that abuses of power because where people are acting harmfully with symmetrical power, they can kind of check it themselves where someone with asymmetric power over someone else is acting harmfully it's very hard to check so how do we create checks and balances.

There's a lot of right thinking in this but total advancement in technological capacity and population size and many things have made it to where those systems as they were put in place not the principles but the systems have mostly broken. 

## New Governance Systems

If we were trying to build a open society a kind of democratic or participatory governance system from scratch today in the 21st century, we wouldn't be thinking about people meeting physically in a town hall that can't possibly hold the population of a local area to have a representative who has to ride a horse. We wouldn't be thinking of that we'd be thinking about how to use the internet to create town halls big enough that everybody could fit into.

We'd be thinking about since there's too much information to process we'd be thinking about how to use artificial intelligence to process the everybody's views into forms information compressed forms that we can actually work with. This obviously has to happen how do we take the advanced technology that has fundamentally changed the nature of governance and implement it to build new governance systems aligned with similar principles and evolutions of those principles that we've gained since then mediated through the right capacities.

Currently the idea that rule of law an estate is a way to bind a particular kind of market-driven multi-polar trap but that is of course within the domain of the rule of law within its jurisdiction. So then international tragedy of the commons issues are a different kind of thing because where does the enforcement live becomes a trickier issue.

If we think about the tragedy of the commons issues that operate at a international and particularly at a global level we can see that the world is having a really hard time figuring out how to deal with these. Climate change is a very classic example because every place is utilizing I mean every nation is utilizing hydrocarbon fuels. Given that energy use in GDP correlate and everybody's in a multi-polar trap race for GDP growth because that equals increased optionality for everything else.

This also means the race to use more total energy and which is driving climate change. How do we get out of that well obviously we aren't paying for the real price of energy right we're simply paying for the cost of extraction. My friend [Nate Hagen says really great work on this topic has a very good podcast](https://www.thegreatsimplification.com/) we've done some discussions there if you want to get more into the issues of energy ecology.

We basically pay for the extraction of the hydrocarbon, like what does it cost to mine this barrel of oil and then add some little margin to make a profit. But that's not the cost that it would take if we needed to make those hydrocarbons ourselves would it cost nature to do it as they're fundamentally unrenewable or the cost to the environment of the pollution side of it if we were trying to cost appropriately.

What it would cost for us to make that fuel renewably and not cause environmental externality in the process the price would go up so much and then that as a key input to every industry that markets would as we understand them today fail in every sector. It's a big deal like because that's real but we're incentivized to externalize all the costs because other people are. If we don't we will lose in comparison so an incentivize to use all the energy we possibly can to grow the GDP as much. 

So if we try to make the price of carbon more real we try to drive the cost of carbon up to disincentivize the use of it we so radically disadvantage ourselves in terms of GDP growth that if anyone else if any other major player doesn't also do that then they will gain so much geopolitical advantage military advantage population growth advantage that they will simply win this multi-polar trap. So unless we could get all of the major players to agree to the thing and have the kind of transparency to ensure that they actually were keeping the agreement and then have some kind of enforcement mechanisms where if they didn't we could enforce it.

This is also really tricky because how do you enforce adherence to something of another country that also has nukes right like what what what are you how far are you willing to escalate the thing before you're like now we're just actually not going to use our nukes. So if we do a sanction and you don't back down and so then we do some limited military action you don't back down at some point you just got to say okay we just don't have the ability to enforce this therefore we have to race because if we go all the way in enforcement and you go back we're all doomed.

So transparency, can we see if the other people are keeping the agreement or not the ability to create those agreements the transparency to see if they're being kept in the capacity for enforcement are critical things to be able to bind multi-polar traps at international levels. Whether we're talking about overfishing of the oceans or the dead zones in the oceans or the depletion of topsoil or the depletion of pollinators or climate change or any of those issues that advantage us to continue to externalize the cost to the commons in the near term relative to others also doing it.

It would disadvantage us not to unless we can get over the multi-polar trap we don't solve these issues so this is a kind of environment or tragedy of the commons case we already gave a kind of military arms race case.

## Network Dynamics

I'll give another example of a market case which is basically the race to first mover advantage of new technologies entering the market the race to market network dominance as we mentioned earlier there's kind of a perverse game theory and this is underlying the game theory of multi-polar traps. That those who are oriented to advance a game theoretic opportunity will get a lot further ahead than those that are oriented to prevent a harm particularly those who are oriented to advance an opportunity for themselves as players and willing to externalize the harm cost to the commons they're not responsible for will get much further ahead than those that are willing to bind their own game theoretic advantage to prevent the harm occurring to the commons.

Now we're back to the how does the peaceful tribe not die in relationship to the warring tribe without becoming a warring tribe topic and so let's say we're looking at the advancement of new technologies that could be radically destructive to the world in many different ways. Like let's say particular types of artificial intelligence if I don't really if for whatever reasons whatever cognitive dispositions or whatever I don't really pay attention to the risk or I don't buy it that much and I buy the opportunity i'm going to do better in being able to advance the product quickly sell it to people you know et cetera than the person who's paying much more attention to the risk even if I buy the risks.

If I want to ensure that i'm advancing it in a way that cannot cause those risks i'm going to move a lot slower and do a lot more steps of protection and safety and due diligence for the commons it still means I will lose first mover advantage and specifically when network dynamics are at play network dynamics are very important. We look at say [metcalfe's law](https://www.peterfisk.com/2020/02/metcalfes-law-explains-how-the-value-of-networks-grow-exponentially-there-are-5-types-of-network-effects/) is the entry to understanding network dynamics. 

This is where the value of a particular company or technology or whatever is mediated by interactions between customers or users in that platform the more users that engage with the technology the more the value goes up the value actually goes up more than linearly it
goes up by a second power to the number of users. So let's say we're talking about something like a search engine or a social media or a currency I don't want to use a currency that like 100 stores use where then I have to have like 30 different currencies in my wallet that's pAIn in the ass. 

I would like to be able to use a dollar that everybody takes I don't want to have 50 logins to different social media accounts that I have to remember where only a few friends are in different ones. I'd like to be able to go to a place where everybody is so there's obviously so much advantage to the number of users on the network being able to share value in value exchange that you end up getting a situation where once any particular technology.

Or let's say implementation of technology which is mostly as companies gets to enough market penetration there's kind of an escape velocity where they will then gain more market penetration faster. This is why I believe in the early days paypal was actually paying people to join the platform right because it made sense they got the amount of money that they had to give in coupons or whatever to join the platform was tiny compared to the increased valuation per user.

They followed a second power equation and this is why there are not lots of fintech platforms of comparable size and that's why amazon is bigger than all the other online stores youtube video players facebook social media google search etc so this is a kind of monopoly right.

Fundamentally it's kind of monopoly but that is not the kind of monopoly that we've historically thought of as something that like was a monopoly through government participation. So we have not oriented towards how to actually bind this thing very well and it can happen very quickly as we could see you know facebook and google and youtube
achieved fundamental monopoly status in a very short period of time at a global scale. Or at least a western world scale that's a big deal it's a big deal to realize because then the idea of checks and balances of power within the market is kind of gone right.

So in a world where network dynamics are a thing, then there is an even more intense race for first mover advantage and as rapid growth as possible because if you don't get to that escape velocity somebody else will whoever gets to that escape velocity will dominate everything for a very long time. So that just the network dynamics intensified the market races radically. Of course anything that would slow down my speed to market and my speed to user adoption decreases my chance of being the top of the power law distribution and no other position really matters right?

How to just race in that way is dominating market dynamics in most areas of emerging technologies and particularly you know exponential technologies. So this means AI being developed for a positive purpose that has negative externalities that either nobody's thought of or nobody feels like they can do anything about it. We hope we'll just be able to solve later being developed for a positive purpose that can be repurposed for negative capacities being developed, for a hopefully positive purpose that might become autopoetic and take off in ways that are totally uncontrollable etc right.

Where then of course our speed of advancement with that drives the arms race of everybody else's increased speed of advancement this is also a multi-polar trap. It's a multi-polar trap in the market and given that now these competitions are not just at the level of a local market but all at the level of a global market. Then the total speed of the race and the scale of the competition and the scale of what is affected is just its zenith.

So in the area of the environment and tragedy of the commons and the particularly in the international cases where enforcement is difficult in the area of actual arms races of kind of military technology in the area of market races, where the race to first mover advantage of market dominance ends up being a race to the bottom in some important way.

These are all examples of a multi-polar trap or a situation in which the inability to get out of the perceived competitive dynamic means that if anybody does the up thing. Since you can't prove that they aren't doing it you have to assume that they are means everybody has to race to do it as fast as possible which means a globally worst case scenario across all these axes for everyone. With each actor doing the rational choice that makes most sense to them in the context in the moment.

So this is not a particular person being bad, it's like well it makes sense of course if they are possibly building the AI weapons and we don't want our people to die we should build their weapons. So you have a situation where each person can do what seems like the obligately moral thing, and yet everyone is collectively doing the stupidest thing for the whole possible and this is a kind of collective action failure. Where our inability to create relationships of trust and our inability to coordinate creates not just inefficiencies and duplication but radically destructive orientation on what could be otherwise constructive capacity.

This is one of the most kind of fundamental dynamics underneath most of both what is driving the catastrophic risks and what makes them so hard to solve. There's also a question that could be an orienting question for humanity of how do we progressively better bind and solve multiple traps, particularly the ones that are driving near-term catastrophic risks and this is an area that I think actually has a lot of solutions not a categorical or perfect solution but a lot of solutions.

I think a lot of humanities collective innovative intelligence focused on would make a really huge difference for instance.

## Solving Multi-Polar Traps

We said one of the challenges of solving the multiple trap internationally is the inability to make international agreements. And one of the reasons where either we don't make the international agreement because we it's not even worth making because we're sure they're going to defect on it so why even bother. Or we make the agreement knowing that they're going to defect on it and we're going to defect on it but we're going to say that we're keeping it and we know they're going to say they're keeping it we're going to spy on them and we're going to lie to their spies and all this kind of where that goes.

If the transparency to know what they were actually doing and them to know what we were doing was there the ability to make and keep the agreement, would be fundamentally different so underneath multipolar traps is the perverse game theory to orient towards opaqueness rather than transparency. This is you know the whole nature of how many things are considered trade secrets in the market or national security secrets and you  know all of the different types of security clearance and classification and special compartmentalized information and whatever. Because we're trying to have information advantage information asymmetry advantage on the other side and we don't want them to know what we're doing. 

But we do want to know what they're doing, and so we invest a lot in this but the opaqueness means that the agreements can't be made so the multi-polar trap is the only thing that is left. 

So let's say there's a company called planet labs it's one of the largest private satellite imaging networks in the world and they've already been working on this for different applications and we've talked about more applications that could happen. Let's say that we take a particular kind of problem that drives a global multi-polar trap that can be seen on the surface of the earth we don't have to deal with the deep underground military bases yet.

So let's say we're talking about streams of pollution that are flowing into rivers or things like that if we're talking about real-time video imaging at pretty high of the entire surface of the earth pretty high granular detail. Can we see where the mining waste is being dumped? Can we see where the major plastic in the ocean or the dead zones in the ocean where the affluent is coming from by kind of reversing the time sequence on the images?

Yeah you know satellites are one example of sensor networks but they can operate in that way. Does the ability to do attribution right specific verifiable attribution of where a particular harm is coming from does that increase the capacity to create justice to be able to bind it totally? So is it possible that certain types of forced transparency can make it to where the ability to hide the harms that then have everyone race to do the harms goes away. 

Then the need the fact that they can be shown means there's a need to account for them and better methods of accountability emerge. I think there's a lot that can be done with forced transparency that orients towards a fundamentally better attractor of the game theory space. 

There's a very interesting question I don't know the answer to this but I am intrigued of are there also scenarios where transparency besides being forced in that kind of way like satellites that are looking at the entire world from international space so the law allows them to do that. 

Are there ways in which transparent solutions win game theoretically as a different peak in the adaptive landscape relative to the opaque solutions why the opaque solutions win is very obvious. Surprise attacks help right the ability to advance something where they don't know that we're advancing it we mislead them about it they build a false defense. Like it's obvious why the desire for information asymmetry in that way is there.

I was talking to someone in national security of Sweden who was telling me some very fascinating things and I haven't been able to verify all of them but saying that the of kind of major nations that have meaningful defense capacities Sweden has maybe the most transparent intelligence and military and security apparatus.

Their underlying philosophy was that the major players are going to be effective with their spying and no anyways they don't really get that much information asymmetry that's more of an illusion of control than a reality of it. So Russia and China and the US are going to know anyways and so the huge amount of resource that they would have to put into trying to hide from them is mostly a waste so they're just not even a try. And that if you have to try to hide the national security stuff from the quote-unquote enemy that also means you have to hide it from your citizens because otherwise your citizens don't all know how to keep classification.

Which is why like in this thing pretty much makes democracy impossible in any real sense more than just a simulation at the time of the founding of modern democracies. Particularly i'll mention the united states here the united states had an ocean on both sides to anybody else it had no kind of contiguous rival land contiguous rival. That was a pretty awesome security situation and so the decisions that needed to be made about anything including military things could be shared with the people the people could actually weigh in on them and sharing that information with those citizens didn't instantly mean sharing it with the british or the spanish or anybody else.

Because there was no way to get that information across the ocean quickly and simultaneously once someone else got that information they couldn't launch an attack that quickly they'd have to launch boats that we could see that would take months it would give us time to launch a you know a response. So the ability to share with the citizens was actually viable in that world because of those sets of reasons.

As soon as we get to a world where whatever is being shared with the citizens can be known by rival countries instantly because of electronic you know telecommunications and where they can then launch attacks instantly including ones that are hidden with plausible deniability like cyber attacks or economic attacks or geopolitical positioning let alone missiles. 

Now the threat of sharing information with the citizens meaning sharing with the rivals is way too high therefore more and more things become classified more and more things become national security secrets obviously attacks on our water supply or our energy grid or our food supply could all be national security threats. So we start finding that there's national security secrets everywhere well how do you do democracy if the citizens can't actually know what is going on and what the real considerations are.

Well you can't it's it's a simulation of it this is a real challenge that like we have to think of in a deep way when we consider what does a participatory governance system in the context of the modern world look like? Because it makes sense that there would be a black budget and it makes sense that there would be national security secrets but then how do you verify that those authorities are not corrupt how do you verify that they are actually doing the will of the people or how do you you know how do you create checks and balances on power or adjudicate issues or things like that?

This also relates to the thing we were mentioning earlier about that when even scientific publishing equals increasing catastrophic risk because the scientific publishing creates the capacity for more catastrophic technology simply through information. I don't have to actually give the capacity to manufacture the tech that is easy give the information on how to code a particular thing catastrophic ability increases.

How do we do open sharing of information in a world where other actors and increasingly smaller actors can use that information in increasingly destructive ways? And if you don't then how do you have anything like participatory governance without open access to the information?

This is now why we talk about that the two primary attractors for the world are catastrophes and dystopias that if we continue to advance exponential technologies where more and more actors have capacity to intentionally or accidentally catastrophic technology and that them like the industrial tech cumulative effects. Now not just weaponized or accidental immediate catastrophes but the cumulative externality effects of the technologies continue to advance so rapidly that world orients towards increasing likelihood of catastrophic risk.

To be able to prevent those catastrophic risks means being able to bind the intentionally or even accidentally or even externality driven catastrophic potentials the ability to bind that looks like some very very powerful regulatory capacities of some kind. Whether it's a decentralized network regulation or whether it's a state or whether it's a whatever the whatever it is and then how.

Then most of the ways in which we could build a thing that could do that that thing to have the power to check decentralized exponential attack has to be powerful enough that what can check it. So then you end up getting dystopic solutions.

So a classic example though by no means the only one is I can't build nuclear weapons in my basement it's too hard to build them. But if I can build bio-weapons or I can build AI weapons or cyber weapons or drone weapons in my basement that can produce not only really bad harm but possibly cascading harm or illicit oher actors to do similar things.

In the presence of decentralized catastrophe weapon capability involving no exotic materials, how do we prevent the world from breaking which increases in probability as time goes on without something like a system of surveillance that knows what people are doing in their basements?

It's very hard to imagine how to do that actually and yet it's very hard to imagine how a system of surveillance that can prevent that which is a very understandable thing to want to do an important thing to want to do who who has access to that surveillance. How do the issues of determining right use of that surveillance and the power of that get adjudicated very easy to see that becoming a really dystopian big brother scenario.
 
## Centralising Power

Already in the west many people feel that the approach that the Chinese government is taking to address very real issues that they see as kind of catastrophic to their civilization that we're not addressing well enough and are eroding our civilization we see as dystopic to certain kinds of civil liberties. We can see the way that it's clear that the polarization engine that social media and infotech is that is driven increasing polarization in the united states in particular.

China resolved that very easily by having their central government just start to ban and regulate those technologies right that kids only had access to a subset of tick tock that shows patriotic and educational videos and they have a limit to the amount of minutes per day and per week they can play them in video games and no live streaming. They were this is a huge deal now that's totally a you know something we could call a paternalistic like overreach of state but we can also see that our state is eroding by not doing that and being captured by market forces and broken down by polarization forces.

The social credit system and the iot the AI mediated iot kind of surveillance system makes a lot of sense to prevent catastrophic technologies happening decentrally. It also creates a lot of uncheckable centralized power. So we I would say that when we talk about what is a desirable civilization it becomes a really tricky topic because it's like centrally existential questions to say what is a good civilization. 

We don't have a system like science this is kind of the is ought distinction where science can say what is but not what are we we don't have a as powerful as sciences for being able to understand the objective world which then allows us to create tech objective tech that affects the objective world. We don't have a comparably effective way of studying the subjective and inter-subjective world of being able to say good right and so if you have the ability to affect the world exponentially more powerfully through science and applied sciences technology. 

But you don't have a comparable powerful force for what is good or wise guidance of that technology then what is guiding that technology ends up being game theory right. Game theory is actually like a the closest thing to a scientific to something that is commensurable with the scientific system in terms of what is a good choice which is kind of the social darwinism of a good choice as a choice that doesn't lose game theoretically to other choices that are also seeking maximum advantage.

But we can see that the advance of game theory under the presence of exponential tech in this way self-terminates so that clearly cannot be called a good choice. Yet losing in the near term is also not a good choice so we need something that is option is none of the above. While I think it is important for us to become as good at the type of internal human capacity that can do wisdom and ethics as we are at the capacity to do science and technology and to be able to think about what is a desirable civilization.

I think we can start by agreeing on a couple things that are not desirable and I think that's actually very helpful I think the idea that a civilization that self-terminates is probably doesn't meet the criteria of what a optimal civilization. So that we are wanting to prevent the movements towards cascading catastrophes that seems pretty straightforward. That one that is clearly dystopic meaning that there are such radical asymmetries of power that the freedoms of almost everyone are radically curtailed is also not an ideal case.

So and yet it's very easy to see that exponential tech has the ability to decentralize power as the market cases giving an example of which increases which creates these coordination collective action problems and multipolar traps. So the decentralized catastrophic or the decentralized exponential power creates multipolar traps creates increasing catastrophes so then it also has the ability to centralize power right.

If I could have an entire nation-state in historically the nation-state couldn't be too top-down and too big because it would get fragile because you can't actually see what everyone's doing and control it. But the ability to have sensors everywhere and have no person could make sense of that and you couldn't even trust the chains of command but AI systems could and then getting everybody to spy on each other is tricky. But being able to mediate that through sesame credit type systems I could do that thing so the exponential tech also creates way more powerful autocratic systems at scale you know centralizing power.

Which brings us back to why the multipolar trap is one of the very deep underlying things to work on and this also brings us back to the Swedish example that I did not complete of where transparency could win. So the example he was giving is that rather than invest any resource in opaqueness or you know hiding or cloaking stuff they would just assume that the rivals would find out anyways. As a result they didn't have to keep stuff from their own population in the same way. 

As a result, the population has way higher trust than government as a result way less money has to be spent in campAIgning and convincing people of things and there's more kind of emergent coordination and things like that and the various departments of government because they're and especially of military have more transparency to what each other are doing. 

Because they're transparent whereas in the classification you don't just have overarching classification you have this kind of special compartmentalized information process which means that a general in one area in a general and the other still don't know what's going on in the other domAIn so that both means that duplication can be happening it means that a lack of efficiency of coordination happens. Particularly when there's too much information it kind of doesn't even matter to have access to all the information because nobody can process it this is the infosingularity issue and we'll talk about this more.

But when we have more computational capacities to process large information sets then it actually really does matter having transparent access to all of it we can make progressively better sense of it with the right computational sense making augmentation.

So their ideas the transparency regarding things that would have otherwise been classified doesn't lose us that much it gAIns us a lot of trust in government. Which gains us a lot of discretionary participation of citizenry and it also gAIns us the ability for the various departments of government and military to be in much more coordination with less duplication and less waste. 

Because they have more transparency to what each other are doing it also means that it's much easier to check corruption because they increase transparency and so you have more you know efficiency and integrity and things like that. As a result you lose the little bit of asymmetry of information advantage but you gain a lot of other kinds of advantages so it's a different peak in the adaptive landscape.

I thought this was fascinating when you share this with me and so I asked you know that's cute and all as a country that has eu backing which fundamentally has nato backing you know indirectly. That is not at the head of an arms race do you think something like that could work for say the united states his take was yes actually could work for the united states because the amount of capacity uplift that would occur.

This didn't mean he thought it was inactive the the vested interest against enacting it would be ridiculous but just hypothetically as a thought experiment. If that kind of transparency did happen the amount of duplication that's occurring is huge the amount of corruption and waste that's occurring is huge the ineffectiveness of coordination that could be lifted would be huge.

The ability to start getting increased trust in government and as a result having the democratic and government sector be more coherent aligned with the military sector as opposed to the increasing discoherence in the government public sector. That even if some first attack capabilities were increased because of the information sharing of other parties that our total strategic capacity and response and deterrence would still make it or nobody would mess with that that such a thing could be advanced.

I think this is a fascinating line of inquiry so if say more total strategic military capacity emerged out of the transparency than the opaqueness approach meaning per you know military power per dollar then it would actually drive a race to the top on transparency. Where Russia and China would be oriented to try to do a similar thing because otherwise they'd actually be losing the multipolar trap that is now a race to the top on transparency rather than race to the bottom on opaqueness.

Which is increasing the capacity for international agreement rather than decreasing the capacity and fundamentally addressing the multipolar trap progressively better. If we think about having some forced transparencies through things like international satellite capability and things that and open source intel capacities that kind of mess up the opaque anyways. Can we make the opaqueness increasingly poor as an adaptive strategy the transparency both more forced and more capable of actually winning now.

We start to get a strategy where for the first time in history possibly something wins game theoretically that is also better in fundamental ways regarding its long-term viability that the culture more oriented to peacefulness can actually beat the culture more orange doorfulness. Warring without becoming more warring in all the most problematic ways it can maintain adequate strategic capacity while being oriented in a way that fundamentally is decreasing the types of pathological competition within the system.

As a result and driving races to the top between systems of things that are more positive some and less pathological as a whole. Ultimately whatever you know we I would say threading the needle of something that is neither catastrophes or dystopias and without being able to you know wave a magic wand and do enactment at the level of the whole world at once.

If any group were to do something the thing they would do has to not lose to the rest of the world not doing that thing not just not lose but it also has to influence the rest of the world. Because if let's say we could get some country to do some very enlightened set of practices it still dies from climate change and AI and whatever so long as it doesn't change what the us and russia and china and other places are doing. So it has to actually be able to influence the whole world shifting but it has to do it so it has to basically be able to win in 
some critical influence ways where the nature of what creates the win isn't externalizing harm or driving arms races or whatever.

So it has to both be obsoleting some kinds of destructive game theory while winning at some fundamental types of game theory at the same time. Like so much the threading of the needle has to occur. I think when we go back to the sub dunbar tribes one of the things that mediated their effective protocols was that there was very high transparency which allowed for pretty good coordination and not having collective action failureswithin the group.

There was no real incentive for anyone to be sociopathic or narcissistic because nobody would want sociopaths or narcissists in the system and unless you can like people and hide it that strategy doesn't pay if everybody can like people and hide it that strategy doesn't pay. If everybody part of the chores you're going to clean it up you're going to kick it out of the tribe so that the smallness creates a force transparency creates a situation in which what is best for the tribe and what's best for you are more closely aligned.

I'm not saying perfect, but more closely aligned when the groups get much larger where somebody has the ability to play people off of each other who don't know that that's happening. Because there aren't enough communication channels and be able to you know screw some people over here and then go somewhere else to get a new supply of people. Then of course the ability to hide the effects of what i'm doing i'll create a incentive for sociopathy and things like that.

Which is why we see that those types of power oriented say cluster personality disorders or something like three times more prevalent in c-suites and positions of more power than they are in the general population is that they are adaptive in those environments so they're selected for conditioned incentivized. So of course we can get the high transparency and thus better alignment between the individual agents incentive and the whole in a small environment but that has never scaled.

If we want to be able to scale it are some of the new technologies that allow for certain kinds of transparency and then certain kinds of information processing across larger scale and communication could they facilitate better methods of collective intelligence that create, that notice perverse incentives and as a result of noticing them and being able to create accounting for them and externalities be able to create accounting for them create progressively better incentives and more capable deterrence to align the motivations of individual agents with the whole better.

## Collective Intelligence

I believe that some of the exponential computational technologies in particular that portend kind of the fastest and worst existential risks in many ways also portend the possibility for better coordination systems. I'm not talking about AI disintermediating humans and having some AI singleton run the world I think that's a really bad idea for lots of reasons. I'm talking about AI being able to facilitate human collective intelligence AI amongst other capacities.

Where I don't want artificial intelligence making the decisions by itself in most scenarios I want processes of collective intelligence of humans making it and we can get into obviously people voting yes no on a binary proposition. Where both sides both versions of the proposition suck if it goes through it benefits something and harm something else if it doesn't go through the thing it would benefit is now harmed.

Because the proposition was just designed stupidly to begin with it didn't factor how interconnected everything was so the yes no on it can't not polarize the population like that's. Just a stupid system of collective intelligence right like that's just we can just do much much better where before you make a proposition you actually do the sense making of what are all the interconnected things.

What are all the values you take those as design constraints to go through a better proposition crafting process of what is the best synergistic satisfier with the least theory of trade-offs possible and what are better voting systems than binary that inherently polarize the population. So I think we can do a radically better job of systems of collective intelligence and a radically better job of education of people to be able to participate with these things and an incentive system where as people become more educated in topics they actually get more ability to influence those topics.

We could get into that it's beyond the scope of this initial introduction but one of the issues is that where there's more information that anybody can process right the information singularity there's more journal articles on a topic than any expert can read so nobody can ever be an expert on anything. How do we solve that one idea is we don't and we simply need AI's to run the world the other idea is we have to be able to merge with the AIs through some kind of brain computer interface or something like that.

I would say another version is that when you look at even the current state of generator AI which is really not advanced compared to what will be a year from now or five years from now because it's advancing so rapidly. But you look at the current state which would be say gpt-3 today open AI it can use exclusively natural language input I don't have to be able to program I just speak to it and it understands my words. It does stuff based on understanding the words that's amazing right like go watch the dolly and the gpt3 demos on youtube to just get a sense of what they can currently do because it's mind-blowing.

The only thing more mind-blowing is the speed at which it's advancing the destructive capabilities of this don't take thinking very hard to imagine and they're very near-term the beneficial cases but that are narrow are also really obvious. The omni-beneficial facilitation how do we create better collective intelligence and governance writ large is not as obvious until you really start to think about it but then it is amazing.

I feel and this is not a techno optimist answer i'm very acutely aware that the technology on the current trajectory is most likely catastrophic. What it takes to make the other version of it is value systems that have to be able to bind guide and direct it and influence social systems that have that create the capacity to bind guide and direct markets and technology in a way they don't currently have.

But it is saying that the technology is new capacity and that capacity if rightly directed does make new things possible in the same way that the printing press made democracy possible where it wouldn't have been before. Because without any newspaper and without textbooks you can't have a comprehensively educated and informed society when hand copying a book is so hard and expensive. Only a nobility class with the wealth can have access to it of course that technology of the printing press did make possible different types of coordination than was possible before obviously the internet anybody talked to anybody anywhere in the world made possible different types of coordination make possible and orient it to happen that way or different.

While most people could use their phone to access any information in the world what they end up doing on it is usually not that interesting examples because of the nature of the choice architectures that they're engaging with in their user interfaces. The choice architectures are that the user interfaces that they engage with have goals for them that are other than that person's highest goals for themselves and they're effective.

So this is not AI will save the world, it's also not AI will necessarily destroy the world this is a since we have the power to create such powerful technology we need the orientation to think about what wise application of that technology is. The intersection of comparable wisdom to guide right design and development of that technology that in turn then is developing wisdom and everyone else based on their interface in the same way that facebook can increase.

It's a known thing you can change the algorithm that is affecting what's in people's newsfeed and they get more depressed or suicidal or more body image issues or not or because we're affected by what we untake could we create information platforms that rather than doubling down on my existing bias were oriented to help expose me to information that would that is antithetical to the things that I currently think. But in the most compelling cases so as to increase my kind of dialectical awareness that rather than orient me to more people that are like the people I already know that orient me to people that are most unlike the people I already know to increase the total connectivity of my network.

That rather than maximizing for things like my engagement and time on site it was maximizing for things like as I was showing the capacity to take and synthesize more perspectives the content that did that got upregulated in the algorithm right. Are tere ways that that same type of technology could be applied that would be wisdom advancing of course this sounds scary because like who's idea of wisdom and who's going to control that?

But it's important to get it's already doing it right it's already controlling minds at scale so it's not a question of do we do it or not it's how do we do it since it is happening right and now. The core question of well what how do we adjudicate what right use of the technology when you realize that the technology is not only radically affecting the earth physically but radically affecting our mind society's cultures.

Now this again the the wisdom of gods to deal with the power of them become central what is the right use of that what are wrong uses where should that be bound how do we understand this how do we make sure that the binding a particular application of a problem doesn't make another worse problem. So we don't like a particular kind of partisan-led censorship so we want more free speech. But if a particular approach to free speech also means a lot more ubiquitous deep fakes and an upregulation of the worst voices because they get most tension that creates eliciting of violence and the breakdowns of democracy it's like okay the obvious answers are all wrong right. 

Because the problem space is more interconnected and more complex so we have to hold all those problem spaces together and think about it. But we come back to this you know gpt3 like AI type tech solving the info singularity. So when you put something to gpt3 it's not going to find a existing web page for you the way that a search engine would. It can generate bespoke content right you can say write me an article in the voice of such and such it factors these kinds of facts and orients and towards this kind of conclusion and whatever and it can do that and progressively more and more effectively more and more turing tests passing across more domains.

What if that just becomes the future of search right where if i'm searching for the information that could inform a particular choice we're wanting to make we're wanting to do something regarding grid security. But grid security I need to know all the things about what really affects good security and how what other environmental and national security and you know et cetera issues are connected to that. Right now I can get billions of search results I can't read billions of search results that's not useful for me.

If I can get the AI read billions of search results find the information that is decision informing based on parameters that i'm specifying and create new bespoke content that is the synthesis of that for incision decision and forming not decision creating the groups of people doing collective intelligence are still making the decisions. But now with the ability to take a synthesized or refined set of more information than they can process it is pre-processed into decision-making information.

Now you say well who controls that algorithm because that's a big deal well what if you can do it lots of different ways right what if you can put different criteria for how to inform it.
In the collective intelligence system all the agents have the capacity to do that kind of thing there are heaps of challenges and problems that we have to solve here.

But this is an example of trap is a coordination failure it's a collective action failure the dunbar number can be thought of as an upper boundary of a particular type of coordination capacity. Where everybody can know everybody and see what everybody's doing talk to everybody. So the collective activity is able to be bound through very high bandwidth communication.

Once we start getting larger than that early empires we got command and control hierarchies and we started to get all the problems that we see in the world today that are now at the scale that we are driving catastrophic risks. Democracies where how do we have a much larger system or rather than have somebody at the top rule. Whether it's one monarch or some small kind of uh nobility class or whatever how do we at least have since we can't get everybody at scale to agree the way that we could maybe get everybody at a tiny scale to agree.

But just a minority agreeing seems like a bummer can we at least get a majority to agree but then of course that thing intrinsically ends up driving polarization in its own decay. So can we make the types of transparency and the types of incentive alignment the types of capacity for deterrence that would exist at a small tribal type scale possible at much much larger scales? 

Where the increase in effective coordination starts to be more effective right you start to reduce a huge amount of waste and duplication and you know failuresof those kinds. So what makes this thing adaptive and selective in a kind of game theoretic situation is also what makes it healthy in terms of the health of the people inside in terms of classes relative people relative to each other and its relationship with the environment.

I think that something like that has to be the answer and I think that it's interesting that the technologies that have the most destructive capability I think also have the most and uniquely facilitative capability for solving collective action or facilitating us solving collective action problems.
